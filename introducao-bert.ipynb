{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao BERT\n",
    "\n",
    "Neste notebook você verá um exemplo de como usar o [BERT](https://arxiv.org/abs/1810.04805) para extrair os embeddings de sentenças, além de conhecer mais sobre este modelo. \n",
    "\n",
    "Este tutorial foi adapato de:  BramVanroy / bert-for-inference (https://github.com/BramVanroy/bert-for-inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bc9d7c76ada7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _C: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O tokenizador (tokenizer)\n",
    "\n",
    "Os modelos deep learning trabalham com tensors, que são basicamente vetores, que por sua vez são um grupo de números. Para começar, o texto de entrada (string) precisa ser convertido em um tipo de data que os modelos possam usar (números). Essa é a tarefa do tokenizador. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializando o tokenizador com um modelo pré-treinado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Durante o pré-treinamento, o tokenizador também é treinado, gerando um vocabulário conhecido. Cada palavra do vocabulário é atribuída a um índice (número), que pode ser usado pelo modelo. \n",
    "\n",
    "Para lidar com problemas das palavras que o tokenizador não conhece (out-of-vocabulary ou OOV), uma técnica é utilizada para garantir que o tokenizador aprenda \"subpalavras\". Desta forma, quando usamos os modelos pré-treinados, não teremos problemas de OOV. Quando o tokenizador não reconhece uma palavra, que não está no vocabulário, ele divide a palavra em pequenas unidades que são conhecidas. O tokenizador do BERT usa [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) para dividir os tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Converte a string \"granola bars\" para um vocabulário tokenizado \n",
    "granola_ids = tokenizer.encode('granola bars')\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))\n",
    "# Converte os IDs to the actual vocabulary item\n",
    "# Notice how the subword unit (suffix) starts with \"##\" to indicate \n",
    "# that it is part of the previous string\n",
    "print('granola_tokens', tokenizer.convert_ids_to_tokens(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Você deve ter notado os tokens especiais [CLS] e [SEP]. Esses tokens são adicionados automaticamente pelo método `.encode()`, então não precisamos nos preocupar com eles. O primeiro é um token de classificação que foi pré-treinado, utilizado nas tarefas de classificação. Desta forma, ao invés de fazer a média de todos os tokens e usá-los como uma representação de frase, é recomendado apenas pegar a saída do [CLS] que representa a frase inteira. [SEP], por sua vez, é inserido como um separador entre várias instâncias, usado por exemplo na predição da próxima sentença, separando uma frase da outra.\n",
    "\n",
    "Como vimos acima, o tipo de dados dos IDs de cada token é uma lista de inteiros. Neste notebook vamos usar a biblioteca `transformers` em combinação com PyTorch, que trabalha com tensores. Um tensor é um tipo especial de lista otimizada, normalmente usado em deep learning. Para converter nossos IDs dos tokens em um tensor, podemos simplesmente chamar um construtor de tensor passando a lista. Aqui, vamos usar um `LongTensor` que é usado para inteiros (para números de ponto flutuante,  usaríamos um `FloatTensor` ou apenas` Tensor`). \n",
    "\n",
    "O método `.encode ()` do tokenizer pode retornar um tensor em vez de uma lista, passando o parâmetro `return_tensors = 'pt'`, mas para fins de ilustração, faremos a conversão de uma lista para um tensor manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of IDs to a tensor of IDs \n",
    "granola_ids = torch.LongTensor(granola_ids)\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## O modelo\n",
    "Agora que pré-processamos nosso texto de entrada em um tensor de IDs (lembrando que cada valor de ID corresponde ao ID do token no vocabulário criado pelo tokenizador ), podemos alimentar o modelo. O modelo sabe qual palavra está sendo processada porque ele sabe qual token pertence a determinado ID. \n",
    "\n",
    "No BERT, assim como na maioria dos modelos de linguagem baseados em Transformers, a primeira camada é uma camada de embedding, cada token possui um embedding relacionado. No BERT, o embedding de um token é a soma de três tipos de embeddings: o embedding do token (gerado para o próprio token), o embedding do segmento (indica se o segmento faz parte da primeira ou o segunda sentença, não usado na inferência de uma única sentença) e o embedding de posição (distingue a posição do token na sentença). \n",
    "\n",
    "Para mais detalhes, veja [esse artigo](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a). \n",
    "\n",
    "Abaixo, uma imagem do BERT retirada do artigo publicado.\n",
    "\n",
    "![BERT embeddings visualization](img/bert-embeddings.png)\n",
    "\n",
    "Para mais explicações sobre o modelo BERT, acesse [Jay Alammar's homepage](http://jalammar.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar, primeiro precisamos inicializar o modelo. Assim como o tokenizer, o modelo é pré-treinado, o que nos permite usar um modelo de linguagem já pré-treinado para obter representações de token ou de senteças.\n",
    "\n",
    "Devemos usar o mesmo modelo pré-treinado que o tokenizer usa (`bert-base-uncased`). Este é o menor modelo BERT, treinado em texto em letras minúsculas. Desta forma, o tokenizer coloca o texto automaticamente em minúsculas para nós. A escolha do modelo, se deve ser caseado ou não, depende da tarefa. Tarefa como NER, por exemplo, podem requerer modelos treinados com maiúsculas e minúsculas.\n",
    "\n",
    "No exemplo abaixo, um argumento adicional foi fornecido para a inicialização do modelo. `output_hidden_states` fornece mais informações de saída. Por padrão, um `BertModel` irá retornar uma tupla, mas o conteúdo dessa tupla é diferente dependendo da configuração do modelo. Ao passar `output_hidden_states = True`, a tupla irá conter:\n",
    "\n",
    "1. O último estado oculto `(batch_size, sequence_length, hidden_size)`\n",
    "2. pooler_output do token de classificação `(batch_size, hidden_size)`\n",
    "3. os estados_ocultos das saídas do modelo em cada camada e as saídas dos embeddings iniciais\n",
    "   `(batch_size, sequence_length, hidden_size)`\n",
    "\n",
    "As placas gráficas (GPUs) são muito melhores em fazer operações em tensores do que uma CPU, portanto, sempre que disponível, executaremos os cálculos em GPU, como a CUDA (para isso, precisaremos de uma versão torch compatível com GPU.) \n",
    "\n",
    "Assim, movemos nosso modelo para o dispositivo correto: se estiver disponível, moveremos o modelo `.to ()` à GPU, caso contrário, permanecerá na CPU. É importante lembrar que o modelo e os dados a serem processados precisam estar no mesmo dispositivo. \n",
    "\n",
    "Finalmente, definimos o modelo para o modo de avaliação (`.eval`), em contraste com o modo de treinamento (` .train () `). Na avaliação, não temos por exemplo o dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "granola_ids = granola_ids.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inferência\n",
    "\n",
    "O modelo foi inicializado e a string de entrada foi convertida em um tensor. Um modelo de linguagem (como\n",
    "`BertModel` acima) tem um método` forward () `que é chamado automaticamente ao chamar o objeto. O método progressivo\n",
    "basicamente empurra um determinado tensor de entrada para frente no modelo e retorna a saída. Já que estamos apenas fazendo\n",
    "inferir e não treinar ou ajustar o modelo, esta é a única etapa que envolve o modelo diretamente para obter\n",
    "resultado. Portanto, não precisamos otimizar o modelo (calcular gradientes, propagando de volta). É muito simples, não é?\n",
    "Uma peculiaridade é que definimos `torch.no_grad ()`. Isso diz ao modelo que não faremos nenhum gradiente\n",
    "cálculo / retropropagação. Em última análise, torna a inferência mais rápida e mais eficiente em termos de memória. Você normalmente usaria\n",
    "`model.eval ()` (veja acima) e `torch.no_grad ()` juntos para avaliação e teste de seu modelo. Ao treinar o\n",
    "model deve ser definido como `model.train ()` e `torch.no_grad ()` * não * deve ser usado.\n",
    "\n",
    "Na célula abaixo, você verá que existe um método estranho chamado `.unsqueeze ()`. Ele \"descomprime\" um tensor adicionando\n",
    "uma dimensão extra. Em nosso caso, você verá que nosso tensor de granola de tamanho `(5,)` se transforma em uma forma diferente de\n",
    "`(1, 5)` onde `1` é a dimensão da frase. Essas duas dimensões são exigidas pelo modelo: ele é otimizado\n",
    "para treinar em * lotes *. O próximo parágrafo entra em um pouco mais de detalhes técnicos, mas não é necessário para entender isso\n",
    "caderno.\n",
    "\n",
    "Um lote consiste em vários textos de entrada \"ao mesmo tempo\" (normalmente da potência de dois, por exemplo, 64). Com um tamanho de lote\n",
    "de 64 (64 frases de uma vez), o tamanho do lote seria `(64, n)` onde `64` é o número de frases e` n` o\n",
    "comprimento da sequência. Neste caderno, onde usamos apenas uma entrada, o seguinte não é importante, mas se você alguma vez\n",
    "deseja ajustar um modelo, você vai querer trabalhar com lotes, uma vez que o cálculo do gradiente será melhor para grandes\n",
    "lotes. Nesses casos, `n` precisa ser o mesmo para todas as entradas; você não pode ter uma sequência de 5 itens e uma de\n",
    "12 itens. É aí que entra o enchimento - mas isso é uma história para outro dia. Por enquanto, você pode se lembrar que o\n",
    "o tamanho de entrada do modelo precisa ser `(n_input_sentences, seq_len)` onde `seq_len` pode ser determinado de diferentes maneiras.\n",
    "Duas escolhas populares são: usar o texto mais longo do lote como `seq_len` (por exemplo, 12) e preencher textos mais curtos até\n",
    "este comprimento, ou definindo um comprimento de sequência máximo fixo para o modelo (normalmente 512) e preencha todos os itens até este\n",
    "comprimento. A última abordagem é mais fácil de implementar, mas não é eficiente em termos de memória e é computacionalmente mais pesada. o\n",
    "a escolha, como sempre, é sua.\n",
    "\n",
    "\n",
    "The model has been initialized, and the input string has been converted into a tensor. A language model (such as \n",
    "`BertModel` above) has a `forward()` method that is called automatically when calling the object. The forward method \n",
    "basically pushes a given input tensor forward through the model and then returns the output. Since we're only doing\n",
    "inference and not training or fine-tuning the model, this is the only step that involves the model directly to get \n",
    "output. So we don't need to optimize the model (calculate gradients, propagating back). That's quite simple, isn't it?\n",
    "One pecularity is that we set `torch.no_grad()`. This tells the model that we won't be doing any gradient \n",
    "calculation/backpropagation. Ultimately, it makes inference faster and more memory-efficient. You would typically use\n",
    "`model.eval()` (see above) and `torch.no_grad()` together for evaluation and testing of your model. When training the\n",
    "model should be set to `model.train()` and `torch.no_grad()` should *not* be used.\n",
    "\n",
    "In the cell below, you'll see that there's a strange method called `.unsqueeze()`. It \"unsqueezes\" a tensor by adding \n",
    "an extra dimension. In our case, you'll see that our granola tensor of size `(5,)` turns into a different shape of\n",
    "`(1, 5)` where `1` is the dimension of the sentence. These two dimensions are required by the model: it is optimised\n",
    "to train on *batches*. The next paragraph goes into a bit more technical detail but is not required to understand this \n",
    "notebook.\n",
    "\n",
    "\n",
    "A batch consists of multiple input texts at \"the same time\" (typically of the power of two, e.g. 64). With a batch size\n",
    "of 64 (64 sentences at once), the batch size would be `(64, n)` where `64` is the number of sentences, and `n` the\n",
    "sequence length. In this notebook, where we only ever use one input, the following is not important, but if you ever\n",
    "want to fine-tune a model, you'll want to work with batches since the gradient calculation will be better for large\n",
    "batches. In such cases, `n` needs to be the same for all entries; you cannot have one sequence of 5 items and one of\n",
    "12 items. That is where padding comes in - but that is a story for another day. For now, you can remember that the\n",
    "input size of the model needs to be `(n_input_sentences, seq_len)` where `seq_len` can be determined in different ways.\n",
    "Two popular choices are: using the longest text in the batch as `seq_len` (e.g. 12) and padding shorter texts up to\n",
    "this length, or setting a fixed maximal sequence length for the model (typically 512) and pad all items up to this\n",
    "length. The latter approach is easier to implement but is not memory-efficient and is computationally heavier. The\n",
    "choice, as always, is yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(granola_ids.size())\n",
    "# unsqueeze IDs to get batch size of 1 as added dimension\n",
    "granola_ids = granola_ids.unsqueeze(0)\n",
    "print(granola_ids.size())\n",
    "\n",
    "print(type(granola_ids))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=granola_ids)\n",
    "\n",
    "# the output is a tuple\n",
    "print(type(out))\n",
    "# the tuple contains three elements as explained above)\n",
    "print(len(out))\n",
    "# we only want the hidden_states\n",
    "hidden_states = out[2]\n",
    "print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "As discussed above, we push the IDs of our input tokens through the `model()`, which internally calls the model's \n",
    "`forward()` method. `out` is a tuple with all relevant output items (see the list that we discussed earlier on). For us\n",
    "the third item in that tuple is the most important one; it contains all `hidden_states` of the model after a forward\n",
    "pass. `hidden_states` is a tuple of the output of each layer in the model for each token. In the previous\n",
    "cell we saw that the tuple contains 13 items. When you execute the cell below, the architecture of the BertModel is\n",
    "shown (from top-down to the bottom). The `hidden_states` include the output of the `embeddings` layer and the output of\n",
    "all 12 `BertLayer`'s in the encoder. The output of each layer has a size of `(batch_size, sequence_length, 768)`.\n",
    "In our case, that is `(1, 5, 768)` because we only have one input string (batch size of 1), and our input string was\n",
    "tokenized into five IDs (sequence length of 5). `768` is the number of hidden dimensions.\n",
    "\n",
    "The critical reader will notice that there is still one more layer after the encoder, called `pooler`, which is not\n",
    "part of `hidden_states`. This layer is used to \"pool\" the output of the classification token but we will not use that \n",
    "here. Its output is returned in the second item of the output tuple `out`, as discussed before.\n",
    "\n",
    "For an in-depth analysis of BERT's architecture, I'd \n",
    "recommend to read [the paper](https://arxiv.org/abs/1810.04805). However, if you like a more visual explanation, \n",
    "[The Illustrated BERT](http://jalammar.github.io/illustrated-bert/) might be a better place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have all hidden_states, we may want to get a usable value out of it. Let's say that we want to retrieve a\n",
    "sentence embedding by averaging over all tokens. In other words, we want to reduce the size of `(1, 5, 768)` to\n",
    "`(1, 768)` where `1` is the batch size and `768` is the number of hidden dimensions. (One could also call `768` the \n",
    "features that you wish to use in another task.) There are many ways to make a sentence abstraction of tokens, and it \n",
    "often depends on the given task. Here, we will take the mean. For now, we will only use the output of the last layer in\n",
    "the encoder, that is, `hidden_states[-1]`. It is important to indicate that we want to take the `torch.mean`\n",
    "_over a given axis_. Since the size of the output of the layers is `(1, 5, 768)`, we want to average over the five \n",
    "tokens, which are in the second dimension (`dim=1`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**We now have a vector of 768 features representing our input sentence.** But we can do more! The BERT paper discusses\n",
    "how they reached the best results by concatenating the output of the last four layers.\n",
    "\n",
    "![BERT embeddings visualization](img/bert-feature-extraction-contextualized-embeddings.png)\n",
    "\n",
    "In our example, that means that\n",
    "we need to get the last four layers of `hidden_states` and concatenate them after which we can take the mean. We want\n",
    "to concatenate across the axis of the hidden dimensions of `768`. As a consequence, our concatenated output vector will\n",
    "be of size `(1, 5, 3072)` where `3072=4*768`, i.e. the concatenation of four layers with a hidden dimension of 768. The\n",
    "concatenated vector is much larger than the output of only a single layer, meaning that it contains a lot more features.\n",
    "Do note, as usual, that it depends on your specific task whether these `3072` features perform better than `768`.\n",
    "\n",
    "Having a vector of shape `(1, 5, 3072)`, we still need to take the mean over the token dimension, as we did before. We\n",
    "end up with one feature vector of size `(3072,)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get last four layers\n",
    "last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "# cast layers to a tuple and concatenate over the last dimension\n",
    "cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "print(cat_hidden_states.size())\n",
    "\n",
    "# take the mean of the concatenated vector over the token dimension\n",
    "cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "print(cat_sentence_embedding)\n",
    "print(cat_sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving and loading results\n",
    "\n",
    "It is likely that you want to use your generated feature vector in another model or task and just save them to your \n",
    "hard drive. You can easily save a tensor with `torch.save` and load it in another script with `torch.load`. Typically,\n",
    "the `.pt` (PyTorch) extension is used. Note that you cannot read the saved file with a text editor. It is a pickled\n",
    "object which allows for efficient (de)compression. If you do want to save your tensors in a readable format, you can\n",
    "convert a tensor to numpy and using something like `np.savetxt('tensor.txt', your_tensor.numpy())`. I do not recommend\n",
    "that approach (I'd stick with `torch.save` or another compression technique) but it is possible.\n",
    "\n",
    "See how we use `.cpu()`? `cpu()` tells PyTorch that we want to move the output tensor back from the GPU to the CPU. \n",
    "This is not a required step, but I think it is good practice when doing feature extraction to move your data to CPU so\n",
    "that when you load it, it is also loaded as a CPU tensor rather than a CUDA tensor. Afterwards you can still move \n",
    "things to GPU if need be, but using CPU by default seems like a good idea. Note that a tensor has to be on CPU if you\n",
    "want to convert it to `.numpy()`, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save our created sentence representation\n",
    "torch.save(cat_sentence_embedding.cpu(), 'my_sent_embed.pt')\n",
    "\n",
    "# load it again\n",
    "loaded_tensor = torch.load('my_sent_embed.pt')\n",
    "print(loaded_tensor)\n",
    "print(loaded_tensor.size())\n",
    "\n",
    "# convert it to numpy to use in e.g. sklearn\n",
    "np_loaded_tensor = loaded_tensor.numpy()\n",
    "print(np_loaded_tensor)\n",
    "print(type(np_loaded_tensor))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
