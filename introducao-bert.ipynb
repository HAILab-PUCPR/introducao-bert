{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao BERT - Gerando *sentence embeddings*\n",
    "\n",
    "Neste *notebook* você verá um exemplo de como usar o [BERT](https://arxiv.org/abs/1810.04805) para extrair os *embeddings* de sentenças, além de conhecer mais sobre este modelo. \n",
    "\n",
    "Fontes:  \n",
    "\n",
    "- [BramVanroy/bert-for-inference](https://github.com/BramVanroy/bert-for-inference).\n",
    "- [Hugging Faces - Github](https://github.com/huggingface/transformers) e [Hugging Faces - site](https://huggingface.co/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O tokenizador (*tokenizer*)\n",
    "\n",
    "Os modelos *deep learning* trabalham com tensors, que são basicamente vetores, que por sua vez são um grupo de números. Para começar, o texto de entrada (*string*) precisa ser convertido em um tipo de data que os modelos possam usar (números). Essa é a tarefa do tokenizador.\n",
    "\n",
    "Para língua portuguesa, podemos usar a versão multilíngue do BERT, ou usar versões pré-treinadas em português brasileiro como o [BERTimbau](https://github.com/neuralmind-ai/portuguese-bert) e [BioBERTpt](https://github.com/HAILab-PUCPR/BioBERTpt), este último treinado no domínio clínico e biomédico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializando o tokenizador com um modelo multilingual pré-treinado\n",
    "# este modelo tem suporte a 100 idiomas, incluindo português \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # versao BERT em inglês\n",
    "\n",
    "# para usar um modelo BERT especifico para portugues, primeiro baixe o modelo\n",
    "# fazer download do modelo e coloque em 'path/to/bert_dir'\n",
    "#model = BertModel.from_pretrained('path/to/bert_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Durante o pré-treinamento, o tokenizador também é treinado, gerando um vocabulário conhecido. Cada palavra do vocabulário é atribuída a um índice (número), que pode ser usado pelo modelo. \n",
    "\n",
    "Para lidar com problemas das palavras que o tokenizador não conhece (*out-of-vocabulary* ou OOV), uma técnica é utilizada para garantir que o tokenizador aprenda \"subpalavras\". Desta forma, quando usamos os modelos pré-treinados, não teremos problemas de OOV. Quando o tokenizador não reconhece uma palavra, que não está no vocabulário, ele divide a palavra em pequenas unidades que são conhecidas. O tokenizador do BERT usa [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) para dividir os tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granola_ids [101, 36960, 10107, 10102, 11121, 14367, 102]\n",
      "tipos de granola_ids <class 'list'>\n",
      "granola_tokens ['[CLS]', 'barra', '##s', 'de', 'gran', '##ola', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Converte a string \"barras de granola\" para um vocabulário tokenizado \n",
    "granola_ids = tokenizer.encode('barras de granola')\n",
    "# Imprime os IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('tipos de granola_ids', type(granola_ids))\n",
    "# Converte os IDs para o item do vocabulário\n",
    "# As subpalavras (sufixo) começam com \"##\", indicando que é uma parte da palavra anterior\n",
    "print('granola_tokens', tokenizer.convert_ids_to_tokens(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Tokens* especiais\n",
    "\n",
    "Você deve ter notado os *tokens* especiais [CLS] e [SEP]. Esses *tokens* são adicionados automaticamente pelo método `.encode()`, então não precisamos nos preocupar com eles. O primeiro é um *token* de classificação que foi pré-treinado, utilizado nas tarefas de classificação. Desta forma, ao invés de fazer a média de todos os *tokens* e usá-los como uma representação de frase, é recomendado apenas pegar a saída do [CLS] que representa a frase inteira. [SEP], por sua vez, é inserido como um separador entre várias instâncias, usado por exemplo na predição da próxima sentença, separando uma frase da outra.\n",
    "\n",
    "### *Tensor*\n",
    "\n",
    "Como vimos acima, o tipo de dados dos IDs de cada *token* é uma lista de inteiros. Neste *notebook* vamos usar a biblioteca `transformers` em combinação com [*PyTorch*](https://pytorch.org/), que trabalha com tensores. Um tensor é um tipo especial de lista otimizada, normalmente usado em *deep learning*. Para converter nossos IDs dos *tokens* em um tensor, podemos simplesmente chamar um construtor de tensor passando a lista. Aqui, vamos usar um `LongTensor` que é usado para inteiros (para números de ponto flutuante,  usaríamos um `FloatTensor` ou apenas` Tensor`). \n",
    "\n",
    "O método `.encode ()` do tokenizer pode retornar um tensor em vez de uma lista, passando o parâmetro `return_tensors = 'pt'`, mas para fins de ilustração, faremos a conversão de uma lista para um tensor manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granola_ids tensor([  101, 36960, 10107, 10102, 11121, 14367,   102])\n",
      "tipos de granola_ids <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Converte a lista de IDs para um tensor de IDs \n",
    "granola_ids = torch.LongTensor(granola_ids)\n",
    "# Imprime os IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('tipos de granola_ids', type(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## O modelo\n",
    "Agora que pré-processamos nosso texto de entrada em um tensor de IDs (lembrando que cada valor de ID corresponde ao ID do *token* no vocabulário criado pelo tokenizador), podemos alimentar o modelo. O modelo sabe qual palavra está sendo processada porque ele sabe qual *token* pertence a determinado ID. \n",
    "\n",
    "No BERT, assim como na maioria dos modelos de linguagem baseados em [Transformer](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), a primeira camada é uma camada de *embedding*, cada *token* possui um *embedding* relacionado. No BERT, o *embedding* de um *token* é a soma de três tipos de *embeddings*: o *embedding* do *token* (gerado para o próprio *token*), o *embedding* do segmento (indica se o segmento faz parte da primeira ou da segunda sentença, não usado na inferência de uma única sentença) e o *embedding* de posição (distingue a posição do *token* na sentença). \n",
    "\n",
    "Para mais detalhes, veja [esse artigo](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a). \n",
    "\n",
    "Abaixo, uma imagem do BERT retirada do artigo publicado.\n",
    "\n",
    "![BERT embeddings visualization](https://github.com/HAILab-PUCPR/introducao-bert/blob/main/imagens/bert-embeddings.png?raw=true)\n",
    "\n",
    "Para mais explicações sobre o modelo BERT, acesse [Jay Alammar's homepage](http://jalammar.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializando o modelo\n",
    "\n",
    "Para começar, primeiro precisamos inicializar o modelo. Assim como o tokenizer, o modelo é pré-treinado, o que nos permite usar um modelo de linguagem já pré-treinado para obter representações de *token* ou de senteças.\n",
    "\n",
    "Devemos usar o mesmo modelo pré-treinado que o tokenizer usa (neste caso, `bert-base-multilingual-uncased`). Este é o modelo multilíngue BERT, treinado em texto em letras minúsculas. Desta forma, o tokenizer coloca o texto automaticamente em minúsculas para nós. A escolha do modelo, se deve ser caseado ou não, depende da tarefa. Tarefa como NER, por exemplo, podem requerer modelos treinados com maiúsculas e minúsculas (neste caso, troque \"*uncased*\" por \"*cased*\").\n",
    "\n",
    "No exemplo abaixo, um argumento adicional foi fornecido para a inicialização do modelo. `output_hidden_states` fornece mais informações de saída. Por padrão, um `BertModel` irá retornar uma tupla, mas o conteúdo dessa tupla é diferente dependendo da configuração do modelo. Ao passar `output_hidden_states = True`, a tupla irá conter:\n",
    "\n",
    "1. O último estado oculto `(batch_size, sequence_length, hidden_size)`\n",
    "2. *pooler_output* do *token* de classificação `(batch_size, hidden_size)`\n",
    "3. os estados_ocultos das saídas do modelo em cada camada e as saídas dos *embeddings* iniciais\n",
    "   `(batch_size, sequence_length, hidden_size)`\n",
    "\n",
    "### GPU x CPU\n",
    "\n",
    "As placas gráficas (GPUs) são muito melhores em fazer operações em tensores do que uma CPU, portanto, sempre que disponível, executaremos os cálculos em GPU, como a CUDA (para isso, precisaremos de uma versão *torch* compatível com GPU.) \n",
    "\n",
    "Assim, movemos nosso modelo para o dispositivo correto: se estiver disponível, moveremos o modelo `.to ()` à GPU, caso contrário, permanecerá na CPU. É importante lembrar que o modelo e os dados a serem processados precisam estar no mesmo dispositivo. \n",
    "\n",
    "Finalmente, definimos o modelo para o modo de avaliação (`.eval`), em contraste com o modo de treinamento (` .train () `). Na avaliação, não temos por exemplo o *dropout*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased', output_hidden_states=True)\n",
    "\n",
    "# Seta o dispositivo para GPU (cuda) se disponível, senão usa CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "granola_ids = granola_ids.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inferência\n",
    "\n",
    "O modelo foi inicializado e a string de entrada (\"granola_ids\") foi convertida em um tensor. Os modelos de linguagem (como\n",
    "`BertModel`, usado acima) possuem um método` forward () `, chamado automaticamente ao chamar o objeto. Esse método envia o tensor de entrada para frente no modelo e retorna a saída. \n",
    "\n",
    "Como aqui trata-se de inferência, e não do treinamento ou ajuste (*fine-tuning*) do modelo, esta é a única etapa em que chamamos o modelo esperando uma saída (*output*). Portanto, não precisamos otimizar o modelo como calcular gradientes e fazer o  *backpropagation*. \n",
    "\n",
    "Definimos `torch.no_grad ()` na inferência para informar ao modelo que não faremos nenhum cálculo de gradiente e/ou retropropagação, tornando a inferência mais rápida e mais eficiente em termos de memória. \n",
    "\n",
    "Geralmente os métodos `model.eval ()` (veja acima) e `torch.no_grad ()` são usados juntos para avaliação e teste do modelo. Para treinar o modelo usamos o método `model.train ()` e o método `torch.no_grad ()` **não** deve ser usado.\n",
    "\n",
    "### Lote (*batch*)\n",
    "\n",
    "Abaixo, veremos um método chamado `.unsqueeze ()`, que \"descomprime\" um tensor adicionando uma dimensão extra. Então, nosso tensor de granola de tamanho `(7,)` irá se transformar em um tensor de `(1, 7)`, onde `1` é a dimensão da frase. Essas duas dimensões são requeridas pelo modelo: ele é otimizado para treinar em **lotes** (*batches*), como veremos adiante.\n",
    "\n",
    "Um lote consiste em vários textos de entrada \"ao mesmo tempo\" (geralmente em potência de dois, por exemplo, 64). Com um tamanho de lote de 64 (ou seja, 64 frases de uma vez), o tamanho do lote seria `(64, n)` onde `64` é o número de frases e ` n` o\n",
    "comprimento da sequência. Aqui, onde usamos apenas uma entrada, isso não é importante, mas ao ajustar um modelo, precisamos trabalhar com lotes, pois o cálculo do gradiente será melhor para grandes lotes. \n",
    "\n",
    "Nesses casos, `n` precisa ser o mesmo para todas as entradas, ou seja, não é possível ter uma sequência de 7 itens e uma de\n",
    "12 itens (para lidar com isso, usamos técnicas de *padding*). O tamanho de entrada do modelo precisa ser `(n_input_sentences, seq_len)` onde `seq_len` pode ser determinado de diferentes maneiras.\n",
    "\n",
    "Duas escolhas populares são: usar o texto mais longo do lote como `seq_len` (por exemplo, 12) e preencher textos mais curtos até\n",
    "este comprimento, ou definir um comprimento de sequência máximo fixo para o modelo (normalmente 512) e preencher todos os itens até este comprimento. A última abordagem é mais fácil de implementar, mas não é eficiente em termos de memória e é computacionalmente mais pesada. Fica a seu critério."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n",
      "torch.Size([1, 7])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(granola_ids.size())\n",
    "# descomprimir IDs para obter o tamanho do lote = 1 como dimensão extra\n",
    "granola_ids = granola_ids.unsqueeze(0)\n",
    "print(granola_ids.size())\n",
    "\n",
    "print(type(granola_ids))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=granola_ids)\n",
    "\n",
    "# a saída é uma tupla\n",
    "print(type(out))\n",
    "# a tupla contém três elementos, que serão explicados abaixo\n",
    "print(len(out))\n",
    "# aqui serão listados apenas os estados ocultos do modelo (hidden_states)\n",
    "hidden_states = out[2]\n",
    "##print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Estado oculto (*hidden state*)\n",
    "\n",
    "Como visto acima, nós enviamos os IDs de nossos *tokens* de entrada por meio do método `model ()`, que chama internamente o\n",
    "método `forward ()`. O `out` é uma tupla com todos os itens de saída relevantes, sendo o terceiro o mais importante, pois contém todos os estados_ocultos (`hidden_states`) do modelo após a execução de um *forward*. \n",
    "\n",
    "`hidden_states` é uma tupla da saída de cada camada no modelo para cada *token*. Na execução anterior, vimos que a tupla contém 13 itens. Quando você executa `print(model)` (célula abaixo), a arquitetura do BertModel é exibida (todas as camadas, de cima para baixo). O `hidden_states` inclui a saída da camada `embeddings` e a saída de todos os 12 `BertLayer` no codificador. A saída de cada camada tem um tamanho de `(batch_size, sequence_length, 768)`.\n",
    "\n",
    "Em nosso exemplo, isso é `(1, 7, 768)` porque temos apenas uma string de entrada (tamanho do lote = 1), e nossa string de entrada foi tokenizada em sete IDs (comprimento de sequência de 7). `768` é o número de dimensões ocultas.\n",
    "\n",
    "Como podemos ver, há mais uma camada após o codificador, chamada `pooler`, que não faz parte dos `hidden_states`. Esta camada é usada para \"agrupar\" a saída do *token* de classificação. Sua saída é retornada no segundo item da tupla de saída `out`, conforme visto antes.\n",
    "\n",
    "Para mais informações sobre a arquitetura do BERT, leia [o artigo](https://arxiv.org/abs/1810.04805) e acesse o conteúdo\n",
    "[The Illustrated BERT](http://jalammar.github.io/illustrated-bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Incorporação de sentença (*sentence embeddings*)\n",
    "\n",
    "Agora que temos todos os `hidden_states`, podemos utilizá-lo em algumas tarefas. Por exemplo, para recuperar uma incorporação de frases (*sentence embeddings*) calculando a média de todos os *tokens*. Ou seja, vamos reduzir o tamanho de `(1, 7, 768)` para\n",
    "`(1, 768)` onde `1` é o tamanho do lote e` 768` é o número de dimensões ocultas. \n",
    "\n",
    "Há diversas maneiras de fazer uma abstração de frase de *tokens*, dependendo da tarefa de PLN. Aqui, estamos usando a média. Por enquanto, usaremos apenas a saída da última camada do codificador, isto é, `hidden_states [-1]`. É importante indicar que queremos pegar o `torch.mean`_sobre um determinado eixo_. Uma vez que o tamanho da saída das camadas é `(1, 7, 768)`, queremos fazer a média sobre os sete *tokens*, que estão na segunda dimensão (`dim = 1`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-8.8275e-03,  7.1126e-02, -1.1254e-01,  2.5045e-01, -2.2547e-01,\n",
      "         4.8724e-01, -5.5254e-02, -3.3521e-01, -4.5242e-01,  2.6642e-01,\n",
      "        -5.4808e-01, -1.4807e-02, -1.1958e-02, -1.7070e-01,  3.4750e-01,\n",
      "        -1.7532e-01,  5.3843e-01, -1.8029e-01,  1.2748e-01,  3.0856e-02,\n",
      "        -2.2213e-01, -1.5565e-01, -1.9474e-02, -1.0924e-01,  4.0263e-02,\n",
      "        -2.2810e-02,  4.4041e-01,  2.1338e-01, -1.6954e-02,  8.1423e-02,\n",
      "         2.6963e-01, -8.9791e-02,  1.3342e-01,  3.0469e-02,  7.9779e-02,\n",
      "        -3.0330e-01, -6.7027e-02,  3.4837e-01,  1.7025e-01, -2.5633e-01,\n",
      "        -2.3061e-02,  2.7046e-01, -1.0953e-01,  3.9915e-01,  2.0829e-01,\n",
      "         1.4845e-01, -1.2066e-01,  1.5884e-01,  2.6394e-01, -2.5287e-01,\n",
      "        -3.7953e-02, -1.4850e-01, -2.3999e-01, -6.9923e-02,  3.0408e-01,\n",
      "        -4.2778e-02, -2.4120e-01, -2.8455e-01, -1.8484e-01,  1.3302e-01,\n",
      "         2.7918e-01, -3.4697e-02,  2.8486e-02, -2.2839e-01, -1.2637e-01,\n",
      "         5.0465e-02,  1.9249e-01,  1.9632e-01, -2.5868e-01, -1.7313e-01,\n",
      "        -3.0875e-01, -1.6329e-01, -6.2465e-01, -1.3696e-01, -1.8967e-01,\n",
      "         1.2806e-01, -7.2381e-02,  2.2660e-01,  8.9643e-02,  2.6235e-02,\n",
      "        -6.6801e-02, -4.4512e-02,  2.2203e-01,  2.9197e-01, -8.5026e-02,\n",
      "         8.6931e-04, -4.3681e-02,  2.3287e-01,  2.8352e-01,  3.7347e-01,\n",
      "         1.2590e-01,  1.3314e-01, -3.1550e-01,  8.1118e-02,  9.2748e-02,\n",
      "        -2.7028e-01, -6.2292e-01,  5.1460e-01, -1.6095e-01,  1.2250e-01,\n",
      "        -2.4623e-01,  3.9516e-02,  1.7382e-01, -2.1623e-01,  4.4164e-01,\n",
      "        -2.4453e-01, -4.8247e-01, -2.0253e-01,  2.9223e-02, -1.9031e-01,\n",
      "         4.1528e-02, -1.7048e-01, -4.2613e-01, -5.0727e-01, -1.1422e-01,\n",
      "         4.1197e-01, -3.6330e-01,  1.0115e-01, -1.0150e-01,  4.0330e-02,\n",
      "        -3.7277e-01,  4.3252e-02,  4.4891e-02,  2.2261e-01, -6.0633e-03,\n",
      "        -2.3361e-02, -6.8514e-02,  3.7341e-01,  4.0452e-01,  1.6509e-01,\n",
      "         3.8642e-02, -3.0467e-02,  5.0324e-01,  7.6617e-02, -6.4776e-02,\n",
      "         9.5350e-02,  1.3635e-01, -9.9317e-02,  3.5909e-01,  2.2984e-01,\n",
      "         3.4685e-01, -6.2505e-01, -8.0809e-02,  5.4973e-02,  4.0314e-01,\n",
      "         2.1418e-01, -7.3899e-02, -3.3724e-01, -1.2083e-01,  4.9555e-01,\n",
      "         1.2796e-01, -2.2017e-01,  7.3957e-02, -1.8549e-01, -5.8403e-01,\n",
      "         1.1992e-01, -1.1621e-01, -1.7105e-01,  1.0870e-01, -7.5124e-01,\n",
      "        -2.1849e-01,  2.3195e-01, -3.8622e-01,  2.3834e-01,  3.2385e-01,\n",
      "        -4.5431e-01, -1.0177e-01,  1.0883e-01,  2.5757e-02, -3.8013e-01,\n",
      "         1.3180e-01, -2.9914e-04, -3.8319e-02,  1.0802e-01, -2.4173e-01,\n",
      "         6.2926e-02,  1.6175e-01,  5.2653e-01,  2.7800e-02,  1.1009e-01,\n",
      "        -4.6203e-02,  1.0944e-01, -4.4271e-01,  3.6249e-01, -1.3954e-02,\n",
      "         9.8583e-02, -2.1147e-01, -3.1630e-01,  5.0216e-02, -1.7394e-02,\n",
      "        -1.5989e-01, -9.7068e-03,  8.0266e-02, -2.2180e-01, -2.2527e-01,\n",
      "        -3.4014e-01, -7.8855e-02,  1.5088e-01, -5.7192e-04,  1.6731e-01,\n",
      "        -1.8026e-01,  2.8868e-01, -1.5779e-01, -4.5959e-02,  7.6064e-02,\n",
      "        -2.0051e-01, -1.4247e-01, -2.8641e-01,  6.3692e-02, -1.9236e-01,\n",
      "         5.3557e-01,  2.9716e-02, -2.6928e-01, -1.0157e-01, -2.6213e-01,\n",
      "        -2.0378e-01, -3.0686e-02,  1.2231e-01,  2.5950e-01,  2.0896e-01,\n",
      "        -4.5323e-01,  2.0180e-01,  7.2974e-02, -6.5673e-03, -3.3378e-01,\n",
      "        -4.1635e-02,  1.3351e-01,  4.3022e-02, -1.2260e-01, -3.0257e-01,\n",
      "         1.1438e-01,  5.4723e-02, -3.6702e-02,  1.7042e-01,  2.1668e-01,\n",
      "        -2.4671e-01, -1.5173e-01, -1.0922e-01, -3.3280e-01,  1.1265e-01,\n",
      "         2.4644e-01, -5.4271e-01, -2.9680e-01,  3.4587e-01,  1.0985e-01,\n",
      "         6.3680e-02, -2.7641e-01,  1.8446e-01,  1.8064e-01, -1.5062e-01,\n",
      "         9.7645e-02,  4.4878e-01, -8.7257e-02,  1.3864e-01,  1.3108e-02,\n",
      "         2.4124e-01, -1.0841e-01, -1.7624e-01,  2.1401e-01,  4.9848e-02,\n",
      "        -2.3919e-01, -1.3122e-01, -2.4274e-01, -5.1117e-02, -4.9569e-02,\n",
      "        -1.9904e-01,  3.1714e-01, -3.9859e-02,  1.4107e-01, -4.1956e-01,\n",
      "        -1.1679e-01, -1.1166e-01,  3.1934e-01, -2.4302e-01,  4.3885e-02,\n",
      "        -3.3262e-02,  6.6943e-02, -1.2444e-01,  2.9848e-01, -2.3320e-01,\n",
      "        -4.0565e-01, -3.7184e-01,  1.3218e-02,  1.2736e-01,  1.8626e-01,\n",
      "        -2.1159e-01, -1.3632e-01, -2.5691e-01, -1.3110e-02, -1.3268e-01,\n",
      "        -5.6758e-01,  1.4088e-01,  4.4962e-01, -2.3038e-01, -2.0201e-01,\n",
      "        -3.0240e-01, -1.3239e-01,  4.4204e-02,  1.4073e-01,  1.9718e-01,\n",
      "         3.8207e-01,  1.3805e-01,  6.5325e-02,  2.0670e-02, -1.1210e-01,\n",
      "         1.0816e-02, -2.6340e-01,  1.8275e-01, -1.3211e-02, -1.0181e-01,\n",
      "        -7.6583e-02, -1.2123e-01, -9.8979e-03,  3.1089e-02, -1.9996e-01,\n",
      "         2.8096e-01, -3.1172e-02, -3.6178e-01, -6.3918e-02,  1.5748e-01,\n",
      "         3.0823e-01, -2.1493e-01, -2.0870e-02, -3.4899e-01, -2.6965e-01,\n",
      "        -8.9576e-02,  1.1414e-01, -7.5717e-01,  1.3373e-01,  3.0146e-01,\n",
      "         7.0972e-02, -6.2096e-02, -2.6741e-01, -4.8794e-01,  2.4126e-01,\n",
      "         1.0112e-01,  2.6792e-01,  3.4213e-02, -2.9626e-02, -1.8781e-01,\n",
      "         1.4687e-01, -2.4799e-01,  7.2552e-01, -2.0020e-01, -1.2303e-01,\n",
      "        -3.1466e-01,  6.3839e-02,  5.3063e-02, -5.0572e-01,  3.3187e-02,\n",
      "        -2.8401e-01,  2.2560e-01,  1.6224e-01, -2.2012e-01,  3.4522e-01,\n",
      "        -2.1632e-01, -1.9295e-01,  1.8827e-01, -3.9186e-02,  3.7110e-01,\n",
      "        -5.6408e-01,  3.4154e-01,  1.9460e-01, -3.9729e-01,  1.8212e-01,\n",
      "        -7.3780e-02, -5.0179e-02,  8.2854e-02, -3.1180e-01, -2.6989e-01,\n",
      "        -1.0747e-01,  1.8865e-01, -2.0902e-01,  4.2398e-01,  2.5688e-01,\n",
      "        -3.2623e-02, -2.1692e-01,  4.2157e-01,  1.3287e-01, -2.4918e-01,\n",
      "         9.5735e-02, -4.0782e-02,  2.3166e-01,  3.7389e-01,  3.6772e-01,\n",
      "         1.3653e-01, -6.4903e-01,  1.2873e-01, -4.3225e-01,  3.8697e-01,\n",
      "         2.5325e-01, -5.3605e-02, -1.8485e-02,  1.0260e-01, -5.5994e-02,\n",
      "        -2.8713e-01,  4.3564e-02, -9.2712e-02, -1.5600e-01,  2.5313e-01,\n",
      "        -2.2497e-01,  3.5522e-02, -4.2954e-01,  1.2643e-01, -7.9228e-02,\n",
      "         1.2659e-01,  1.2896e-01,  3.9361e-01, -5.5538e-03, -4.7952e-01,\n",
      "        -5.4573e-01,  4.9745e-02,  1.0316e-02, -1.2004e+00, -3.3456e-02,\n",
      "         7.3313e-02, -1.7580e-01,  2.5605e-01,  8.8477e-03,  4.0489e-01,\n",
      "         7.3113e-02, -2.3052e-01,  1.1513e-01, -1.4325e-01, -2.5127e-01,\n",
      "        -1.6901e-01,  1.5778e-01,  4.6672e-01,  4.3135e-01,  1.6827e-01,\n",
      "        -1.6362e-01, -2.1363e-01,  3.3585e-01, -3.0011e-02, -4.7377e-01,\n",
      "         8.4149e-02,  1.3986e-01,  4.2708e-03,  2.9278e-01, -1.7043e-01,\n",
      "         7.0046e-02,  1.9798e-01,  2.8064e-03, -1.2625e-01,  1.4352e-01,\n",
      "         3.9994e-01,  8.2190e-01,  1.8573e-01,  1.5117e-01,  1.9632e-01,\n",
      "         1.6782e-01,  1.8131e-02,  1.0085e-01, -1.1702e-01,  5.4801e-02,\n",
      "         6.7656e-02, -1.4064e-01,  6.4579e-01, -6.6051e-02,  9.3533e-01,\n",
      "        -1.0247e-01,  1.6659e-01,  2.1691e-01,  4.0567e-01, -6.7934e-02,\n",
      "        -1.4471e-01, -3.8983e-01, -6.4574e-02, -1.8164e-01, -3.0317e-01,\n",
      "         8.7563e-01,  1.1168e-01, -6.0139e-01, -4.4461e-03, -1.7309e-01,\n",
      "        -1.0179e-01,  3.1672e-01, -6.8939e-02,  4.6084e-01,  2.9338e-01,\n",
      "        -1.5565e-01, -1.9461e-01,  3.5371e-01,  2.9567e-01,  2.8865e-01,\n",
      "         1.5640e-01,  6.9776e-02,  2.5626e-01,  3.1787e-01, -1.3734e-01,\n",
      "         5.4390e-02, -2.2733e-01,  7.6638e-02,  3.2507e-01, -4.6864e-02,\n",
      "         1.1176e-01,  8.7454e-02, -1.7268e-01,  2.9747e-01,  3.0055e-01,\n",
      "         4.6582e-02,  1.8862e-01, -4.8802e-01, -2.5635e-01,  9.9234e-03,\n",
      "        -3.4908e-01,  7.5339e-02, -2.0473e-01, -2.1153e-01,  2.4246e-01,\n",
      "         1.5273e-01,  3.9379e-01, -2.8242e-01,  1.6901e-01, -4.0048e-01,\n",
      "        -2.5967e-01,  1.0568e-01, -4.3690e-01,  1.0600e-01,  1.2208e-01,\n",
      "        -1.2720e-02, -1.5275e-01,  5.9603e-01, -9.7148e-02, -3.3436e-01,\n",
      "        -4.3677e-02, -9.7755e-02, -2.1524e-01,  3.0505e-01, -2.1928e-02,\n",
      "         2.0522e-02, -4.4680e-02,  6.9336e-01,  2.5145e-01,  2.8076e-02,\n",
      "         3.8798e-01, -5.2358e-02, -7.4194e-02,  2.9641e-01, -1.8054e-01,\n",
      "        -3.5628e-01,  2.0893e-01, -5.5113e-01, -1.1741e-01,  5.0295e-01,\n",
      "         6.7146e-02,  6.8215e-02,  1.8781e-01,  4.6781e-01, -4.6427e-02,\n",
      "        -2.6083e-01,  6.1541e-02,  1.2436e-01,  2.9923e-01,  4.4037e-02,\n",
      "        -5.5774e-01, -1.8575e-02, -3.3993e-01, -5.0385e-01, -7.8905e-02,\n",
      "         1.6445e-01,  9.1116e-02,  3.1541e-01, -1.0808e-01, -1.3961e-01,\n",
      "        -1.4619e-01, -7.8326e-02,  3.3610e-01,  2.0695e-01, -3.5307e-01,\n",
      "         2.8877e-01,  1.1772e-02, -5.1629e-01,  9.0588e-02,  1.7021e-01,\n",
      "        -2.0226e-02, -6.3587e-02,  1.7490e-01, -8.3201e-02, -1.4214e-01,\n",
      "         4.0399e-01, -4.4685e-01, -3.3166e-02,  3.8043e-01,  2.1746e-01,\n",
      "         3.2895e-01, -1.0783e-01, -2.0131e-01, -2.3941e-01, -2.3457e-01,\n",
      "        -5.4020e-01,  1.2780e-01,  2.1235e-01, -2.3245e-01,  5.6535e-01,\n",
      "         6.1330e-02,  2.3571e-01, -2.5729e-02,  3.7180e-01, -7.6937e-02,\n",
      "         7.3191e-02,  2.5715e-01,  2.6740e-01,  1.9827e-01, -4.6445e-02,\n",
      "         3.9251e-01,  2.7966e-01,  1.9537e-01, -1.6154e-01, -3.1870e-01,\n",
      "         1.7690e-01,  8.5734e-02,  3.1074e-01,  3.0917e-01,  3.2182e-01,\n",
      "        -4.0063e-01,  1.5307e-01,  2.3748e-01,  6.0413e-01,  8.5638e-02,\n",
      "         1.3533e-02, -3.1654e-01,  1.3426e-01,  4.2566e-01,  2.0478e-01,\n",
      "        -1.6239e-02,  1.6063e-01,  1.0408e-01, -2.8630e-01,  2.5498e-01,\n",
      "         1.5530e-02,  2.7206e-01, -2.5398e-01,  2.2772e-01,  1.0741e-01,\n",
      "        -1.9903e-01,  5.0653e-02, -1.9130e-01, -3.3597e-01, -7.1498e-02,\n",
      "         9.7069e-01,  2.0233e-01, -3.1850e-01, -4.1591e-02,  2.6842e-01,\n",
      "         3.9444e-02,  1.6223e-01,  4.0973e-01,  5.3539e-02, -2.0337e-01,\n",
      "        -3.1162e-01,  2.8677e-01,  3.1323e-01,  1.0414e-01,  4.3230e-01,\n",
      "        -4.8410e-01,  1.4675e-01,  3.5600e-02,  2.0353e-01,  1.8184e-01,\n",
      "         7.8425e-02,  9.0201e-01, -5.3279e-01,  1.1113e-01,  1.4917e-01,\n",
      "         2.3341e-01, -1.7465e-01, -2.4123e-01,  3.7059e-02, -1.4709e-02,\n",
      "         3.3575e-02, -7.3804e-02, -2.0040e-02,  4.7034e-01, -2.3454e-01,\n",
      "        -4.1656e-01,  2.3062e-01,  5.8929e-02, -1.0979e-01, -7.8032e-02,\n",
      "         1.5186e-01,  3.4644e-01,  1.1174e-01, -1.7944e-01, -2.8274e-02,\n",
      "        -4.3059e-02, -1.7926e-01, -2.4200e-01, -3.2774e-01, -1.0237e-01,\n",
      "        -3.8386e-02,  1.8639e-01, -3.7471e-01,  3.7680e-01, -4.9167e-03,\n",
      "        -4.7563e-01, -1.6722e-01, -3.7409e-02,  2.2548e-01,  2.6555e-01,\n",
      "         7.2114e-01,  3.2660e-01, -6.0651e-02, -6.7299e-02,  1.2530e-01,\n",
      "        -2.5171e-01, -7.7522e-02, -5.7728e-02,  3.2038e-01, -5.3595e-01,\n",
      "        -4.6797e-01,  5.6536e-02, -1.5346e-01, -1.3615e-01, -1.9995e-01,\n",
      "         9.7912e-02, -1.9520e-01,  1.8520e-01, -1.2511e-01,  1.3747e-01,\n",
      "        -2.1374e-01, -1.1961e-01, -1.8795e-01,  1.3572e-01, -7.9872e-02,\n",
      "        -1.1053e-01,  1.0191e-01,  2.3223e-01, -4.0984e-01,  2.3814e-01,\n",
      "        -1.9728e-01,  1.9729e-01,  1.6509e-01,  2.5189e-01,  4.3379e-01,\n",
      "        -4.3653e-02, -1.8079e-01,  1.0451e-01, -5.9863e-02,  3.7469e-01,\n",
      "         2.1608e-01,  1.8252e-02, -1.9660e-01, -4.6012e-01, -2.1815e-01,\n",
      "        -1.4245e-01, -1.9437e-01, -8.8155e-02, -1.5011e-01, -7.2924e-02,\n",
      "        -6.2428e-02, -6.2121e-01,  1.1621e-01, -2.7640e-01, -7.9927e-02,\n",
      "        -1.1393e-01,  4.0248e-02,  1.3988e-01,  5.9457e-01, -8.3922e-02,\n",
      "        -2.5413e-01,  1.8942e-02,  4.3778e-01, -1.1827e-01,  7.8686e-03,\n",
      "         1.8558e-01,  8.2243e-02, -6.1504e-01])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Agora temos um vetor de 768 recursos que representam nossa sentença de entrada.** Mas podemos fazer mais! O artigo do BERT discute como alcançar os melhores resultados concatenando a saída das últimas quatro camadas.\n",
    "\n",
    "![Visualização de embeddings de BERT](https://raw.githubusercontent.com/HAILab-PUCPR/introducao-bert/main/imagens/bert-feature-extraction-contextualized-embeddings.png)\n",
    "\n",
    "Em nosso exemplo, isso significa que precisamos pegar as últimas quatro camadas de `hidden_states`, concatená-los e gerar a média. Nós queremos concatenar no eixo das dimensões ocultas de `768`. Como consequência, nosso vetor de saída concatenado irá\n",
    "ser do tamanho `(1, 7, 3072)` onde `3072 = 4 * 768`, ou seja, a concatenação de quatro camadas com uma dimensão oculta de 768. O\n",
    "vetor concatenado é muito maior do que a saída de apenas uma camada, o que significa que contém muito mais recursos.\n",
    "\n",
    "Para algumas tarefas, esses recursos `3072` podem tem um desempenho melhor do que ` 768`.\n",
    "\n",
    "Tendo um vetor de forma `(1, 7, 3072)`, ainda precisamos obter a média sobre a dimensão do *token*, como fizemos antes, ficando com um vetor de recurso de tamanho `(3072,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 3072])\n",
      "tensor([-0.0088,  0.0711, -0.1125,  ..., -0.1473,  0.0085, -0.2370])\n",
      "torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "# obter as ultimas quatro camadas\n",
    "last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "# juntas as camadas em uma tupla e concatenar com a ultima dimensão\n",
    "cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "print(cat_hidden_states.size())\n",
    "\n",
    "# pegar a média do vetor concatenado sobre a dimensão do token\n",
    "cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "print(cat_sentence_embedding)\n",
    "print(cat_sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Salvando e carregando resultados ##\n",
    "\n",
    "É possível usar o vetor de recurso gerado em outro modelo ou tarefa, para isso basta salvar o tensor com `torch.save` e carregá-lo em outro script com` torch.load`, gerando arquivos na extensão `.pt` (*PyTorch*). Não é possível ler o arquivo salvo com um editor de texto (é um objeto especial que permite uma des(compressão) eficiente). \n",
    "\n",
    "Também é possível salvar os tensores em um formato legível, convertendo em numpy e use algo como `np.savetxt ('tensor.txt', your_tensor.numpy ())`, porém essa abordagem não é recomendada (é melhor usar o `torch.save` ou outra técnica de compressão).\n",
    "\n",
    "Ao usar `.cpu ()`, dizemos ao *PyTorch* que queremos mover o tensor de saída de volta da GPU para a CPU. Isso não é obrigatório, mas é uma boa prática, ao fazer extração de recursos, mover os dados para a CPU. Desta forma, ao carregá-lo, ele é carregado como um tensor de CPU em vez de um tensor CUDA. Depois podemos mover novamente para a GPU, se necessário, mas usar a CPU por padrão é uma boa ideia (o tensor deve estar na CPU para podermos convertê-lo para `.numpy ()`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0088,  0.0711, -0.1125,  ..., -0.1473,  0.0085, -0.2370])\n",
      "torch.Size([3072])\n",
      "[-0.00882755  0.07112557 -0.11253648 ... -0.14732859  0.00853673\n",
      " -0.23699594]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# sava nossa representação de sentença \n",
    "torch.save(cat_sentence_embedding.cpu(), 'my_sent_embed.pt')\n",
    "\n",
    "# faz o load\n",
    "loaded_tensor = torch.load('my_sent_embed.pt')\n",
    "print(loaded_tensor)\n",
    "print(loaded_tensor.size())\n",
    "\n",
    "# converte para numpy para usar por ex com sklearn\n",
    "np_loaded_tensor = loaded_tensor.numpy()\n",
    "print(np_loaded_tensor)\n",
    "print(type(np_loaded_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dica ###\n",
    "Você também poderá usar um serviço para gerar e devolver os *embeddings*, usando [**bert-as-service**](https://github.com/hanxiao/bert-as-service)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
