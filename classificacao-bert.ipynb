{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "jupyter-bert-classificacao.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cb9049145974d55b3e8c9b075c16185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6bc4fa3801b45e189c6e7f3de641c06",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4f3b06cac2094a61b00f9a13b55bfc1c",
              "IPY_MODEL_d00d3f968679442d9f69ba775ae843d1"
            ]
          }
        },
        "e6bc4fa3801b45e189c6e7f3de641c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f3b06cac2094a61b00f9a13b55bfc1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_38e4081b0ee2413a9b525a0fef919d89",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6787ebe6ab364f83b72b90406adc6589"
          }
        },
        "d00d3f968679442d9f69ba775ae843d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a6bcf541fe14666ae9996eee77f5205",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 939kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2c278cfcaca4e718bcbd866769e1842"
          }
        },
        "38e4081b0ee2413a9b525a0fef919d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6787ebe6ab364f83b72b90406adc6589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a6bcf541fe14666ae9996eee77f5205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2c278cfcaca4e718bcbd866769e1842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pieQOy1UThI"
      },
      "source": [
        "# Classificação com BERT\n",
        "\n",
        "Neste *notebook* você verá um exemplo de como usar o [BERT](https://arxiv.org/abs/1810.04805) para tarefa de classificação, usando a bilbioteca **Transformers** do **Hugging Faces**. \n",
        "\n",
        "Fontes:  \n",
        "\n",
        "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/).\n",
        "- [Hugging Faces - Github](https://github.com/huggingface/transformers) e [Hugging Faces - site](https://huggingface.co/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq0Z5iWeUThO"
      },
      "source": [
        "Primeiro, vamos verificar se temos GPU disponível para nossa execução. Não se preocupe caso não possua GPU, apenas o treinamento será mais demorado.\n",
        "\n",
        "Caso você esteja executando no Colab, acesse: Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iH3iyyMUThO",
        "outputId": "86b2e17d-168a-4d63-c5c9-bd37b306740a"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Verificar se tem GPU disponível\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Informa PyTorch para usar GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('Existe(m)  %d GPU(s) disponível(eis).' % torch.cuda.device_count())\n",
        "\n",
        "    print('Vamos usar a GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Se não tem...\n",
        "else:\n",
        "    print('Sem GPU disponível, usando CPU.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Existe(m)  1 GPU(s) disponível(eis).\n",
            "Vamos usar a GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6riA3KZUThQ"
      },
      "source": [
        "### Biblioteca Transformers com *Pytorch*\n",
        "\n",
        "Aqui vamos usar a interface *Pytorch* porque possui um bom equilíbrio entre as APIs de alto nível (fáceis de usar, mas sem fornecer informações sobre como as coisas funcionam) e código de tensorflow (que contém muitos detalhes, mas não tão fáceis de usar).\n",
        "\n",
        "No momento, a biblioteca **Hugging Face** é a interface *Pytorch* mais utilizada para trabalhar com BERT. Além de oferecer suporte a uma variedade de modelos pré-treinados, também inclui modelos especializados para tarefas específicas. Neste tutorial vamos usar *BertForSequenceClassification*.\n",
        "\n",
        "A biblioteca também inclui classes específicas de tarefas para classificação de *tokens*, resposta a perguntas (Q&A), previsão da próxima frase, etc. O uso dessas classes pré-construídas simplifica o processo de modificação de BERT para nossos propósitos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrbPDnszUThQ",
        "outputId": "c6392f9b-06d9-4e2d-a7d4-e753529d8682"
      },
      "source": [
        "# execute essa linha se estiver usando Colab\n",
        "#!pip install transformers\n",
        "!pip install transformers~=2.11.0\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers~=2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers~=2.11.0) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers~=2.11.0) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers~=2.11.0) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers~=2.11.0) (20.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers~=2.11.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers~=2.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers~=2.11.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers~=2.11.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers~=2.11.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers~=2.11.0) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers~=2.11.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers~=2.11.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers~=2.11.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers~=2.11.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers~=2.11.0) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=12193703474a96ecbb9d9aad6b47fc3075c95ff466ca9352a6d3f2fa7c9950a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_pii6c_iUThQ"
      },
      "source": [
        "### Carregando o conjunto de dados (CoLA)\n",
        "\n",
        "Usaremos o conjunto de dados do *Corpus of Linguistic Acceptability (CoLA)* para a classificação de uma única frase. É um conjunto de frases rotuladas como gramaticalmente corretas ou incorretas. Foi publicado pela primeira vez em maio de 2018, e é um dos testes incluídos no “Benchmark GLUE” em que competem modelos como o BERT.\n",
        "\n",
        "Faça *download* do dataset neste link: https://nyu-mll.github.io/CoLA/ e descompacte o conjunto de dados na mesma pasta em que está trabalhando.\n",
        "\n",
        "Ou, se estiver no Colab, use o comando abaixo:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgM7p28fUThR",
        "outputId": "604f5193-edff-49f6-ac86-6365e44075c2"
      },
      "source": [
        "# Usaremos o wget para baixar o conjunto de dados para o sistema de arquivos da instância Colab.\n",
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "import os\n",
        "\n",
        "print('Baixando dataset...')\n",
        "\n",
        "# URL para baixar o dataset em formato zip.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Baixar arquivo (se não temos ainda)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')\n",
        "    \n",
        "# Dezipar o dataset (se ainda não temos)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=35059a9ac2524714809ee10c400fbfd11a49e6ec1c351e78af2f174b204d2546\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Baixando dataset...\n",
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        },
        "id": "U6KkxO5mUThR"
      },
      "source": [
        "### Análise\n",
        "\n",
        "Podemos ver pelos nomes dos arquivos que temos as versões originais e as tokenizadas dos dados.\n",
        "\n",
        "Não vamos usar a versão pré-tokenizada porque, para aplicar o nosso modelo BERT pré-treinado, devemos usar o tokenizador fornecido pelo modelo. Isso porque: (1) o modelo tem um vocabulário específico e fixo e (2) o tokenizador BERT tem uma maneira particular de lidar com palavras fora do vocabulário.\n",
        "\n",
        "Usaremos a biblioteca *pandas* para analisar o conjunto de treinamento e examinar algumas de suas propriedades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "BrpMue0gUThR",
        "outputId": "7f62ff4b-9e81-4f2a-b627-34b9e2168999"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carrega o dataset em um dataframe pandas\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Imprime o número de frases.\n",
        "print('Número de sentenças de treinamento: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Exibe 10 frases randômicas do dataset\n",
        "df.sample(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de sentenças de treinamento: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3335</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There ran through the valley a rushing stream.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2853</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Carol cut her thumb.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Ate Bill his dinner?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2411</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>$5 will buy you a ticket.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8456</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Peter is the old pigs.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6383</th>\n",
              "      <td>d_98</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>You may pick any flower except the rose.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joe likes his bar, and Sally's patrons do too.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3417</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>The phone company billed $10 to me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Oliver addressed the king the letter.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4189</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>One of the story has appeared in your newspaper.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                          sentence\n",
              "3335            l-93  ...    There ran through the valley a rushing stream.\n",
              "2853            l-93  ...                              Carol cut her thumb.\n",
              "5992            c_13  ...                              Ate Bill his dinner?\n",
              "2411            l-93  ...                         $5 will buy you a ticket.\n",
              "8456            ad03  ...                            Peter is the old pigs.\n",
              "6383            d_98  ...          You may pick any flower except the rose.\n",
              "906             bc01  ...    Joe likes his bar, and Sally's patrons do too.\n",
              "3417            l-93  ...               The phone company billed $10 to me.\n",
              "3977            ks08  ...             Oliver addressed the king the letter.\n",
              "4189            ks08  ...  One of the story has appeared in your newspaper.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "oPl1u-8BUThS"
      },
      "source": [
        "As duas propriedades importantes são as frases (*sentence*) e seus rótulos (*label*), que podem ser 0 = incorreta e 1 = correta.\n",
        "\n",
        "Aqui estão cinco frases que são rotuladas como incorretas gramaticalmente (em inglês). Observe como essa tarefa é muito mais difícil do que algo como análise de sentimento!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "J4rp0KUmUThS",
        "outputId": "d2fc88ce-5eed-4663-e831-65083fbabbee"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1387</th>\n",
              "      <td>The place which I live at is the place where R...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3498</th>\n",
              "      <td>A pastor was executed, notwithstanding on many...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8194</th>\n",
              "      <td>I sent she away.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2674</th>\n",
              "      <td>Books send easily to children.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1365</th>\n",
              "      <td>The boy's guardian's we elected employer presi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1387  The place which I live at is the place where R...      0\n",
              "3498  A pastor was executed, notwithstanding on many...      0\n",
              "8194                                   I sent she away.      0\n",
              "2674                     Books send easily to children.      0\n",
              "1365  The boy's guardian's we elected employer presi...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPOPhM5uUThT"
      },
      "source": [
        "Vamos extrair as frases e seus rótulos do nosso corpus de treinamento como *numpy ndarrays*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "Ozgqh2jUUThT"
      },
      "source": [
        "# Lista de sentenças e seus rótulos\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eZrtVcKtUThT"
      },
      "source": [
        "### Tokenização\n",
        "\n",
        "Agora vamos transformar nosso conjunto de dados no formato em que o BERT pode ser treinado.\n",
        "\n",
        "Para alimentar o BERT com nosso texto, ele deve ser dividido em *tokens* e, em seguida, esses *tokens* devem ser mapeados para seu índice no vocabulário do tokenizador.\n",
        "\n",
        "A tokenização deve ser realizada pelo tokenizador incluído no modelo BERT que estamos trabalhando, que vamos baixar no código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "0cb9049145974d55b3e8c9b075c16185",
            "e6bc4fa3801b45e189c6e7f3de641c06",
            "4f3b06cac2094a61b00f9a13b55bfc1c",
            "d00d3f968679442d9f69ba775ae843d1",
            "38e4081b0ee2413a9b525a0fef919d89",
            "6787ebe6ab364f83b72b90406adc6589",
            "7a6bcf541fe14666ae9996eee77f5205",
            "c2c278cfcaca4e718bcbd866769e1842"
          ]
        },
        "id": "N7x158YBUThT",
        "outputId": "7313952b-f603-4e0a-9a70-3fd3adf86b32"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Carregar o tokenizador BERT.\n",
        "print('Carregando tokenizador BERT...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carregando tokenizador BERT...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb9049145974d55b3e8c9b075c16185",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        },
        "id": "vH1IYgJnUThU"
      },
      "source": [
        "Vamos aplicar o tokenizador a uma frase para ver a saída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% \n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF-KTEmyUThU",
        "outputId": "16b0c4c3-721b-471b-e3b6-3d3a43805b3d"
      },
      "source": [
        "# Frase original\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Frase dividida em tokens\n",
        "print('Tokenizada: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Sentença mapeada em token ids\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenizada:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "NIaObrfhUThU"
      },
      "source": [
        "Quando realmente convertermos todas as nossas sentenças, usaremos a função ```tokenize.encode``` para lidar com ambas as etapas, em vez de chamar as funções que usamos acima separadamente.\n",
        "\n",
        "Antes de fazermos isso, precisamos falar sobre alguns dos requisitos de formatação do BERT.\n",
        "\n",
        "### Formatação\n",
        "\n",
        "O código acima omitiu algumas etapas de formatação necessárias que veremos aqui.\n",
        "\n",
        "Precisamos:\n",
        "1. Adicionar *tokens* especiais no início e fim de cada frase.\n",
        "2. Prencher e truncar todas as frases para terem o mesmo comprimento.\n",
        "3. Diferenciar explicitamente *tokens* reais de *tokens* de preenchimento (PAD) com a “máscara de atenção”.\n",
        "\n",
        "**Tokens especiais**\n",
        "[SEP] - No final de cada frase, precisamos acrescentar o token especial [SEP], que é usado em tarefas de duas sentenças, onde BERT recebe duas sentenças separadas e é solicitado a determinar algo (por exemplo, a resposta à pergunta na sentença A pode ser encontrada na sentença B).\n",
        "\n",
        "[CLS] - Para tarefas de classificação, devemos acrescentar o [CLS] ao início de cada frase. Como o BERT consiste em 12 camadas de Tranformer, cada uma recebe uma lista de *embeddings* e produz o mesmo número de *embeddings* na saída (com outros valores), na saída do transformador final (12º), apenas o primeiro *embedding* (correspondente ao *token* [CLS]) é usado pelo classificador .\n",
        "\n",
        "“O primeiro *token* de cada sequência é sempre um *token* de classificação especial ([CLS]), onde o estado oculto final deste *token* é usado como a representação da sequência agregada para tarefas de classificação.” (do artigo BERT)\n",
        "\n",
        "Não precisamos pensar em estratégias de *pool* sobre os *embeddings* finais, pois nesse token [CLS] de classificação, o modelo codificou tudo o que precisamos para a classificação naquele único vetor de incorporação de 768 valores. Já está feito o *pool* para nós!\n",
        "\n",
        "*Comprimento da frase e máscara de atenção*\n",
        "\n",
        "As sentenças em nosso conjunto de dados obviamente têm comprimentos variados, então como o BERT lida com isso?\n",
        "\n",
        "BERT tem duas restrições:\n",
        "\n",
        "- Todas as frases devem ser preenchidas ou truncadas em um único comprimento fixo.\n",
        "- O comprimento máximo da frase é 512 *tokens*.\n",
        "\n",
        "O preenchimento é feito com um *token* especial [PAD], que está no índice 0 no vocabulário BERT.\n",
        "\n",
        "A “Máscara de Atenção” é simplesmente uma matriz de 1s e 0s indicando quais *tokens* são de preenchimento e quais não são (parece meio redundante, não é ?). Essa máscara diz ao mecanismo de “Autoatenção” do BERT para não incorporar esses *tokens* (PAD) em sua interpretação da frase.\n",
        "\n",
        "O comprimento máximo afeta a velocidade de treinamento e avaliação. Por exemplo, com um Tesla K80:\n",
        "\n",
        "MAX_LEN = 128 --> Cada época leva ~5:28 para treinar\n",
        "\n",
        "MAX_LEN = 64 --> Cada época leva ~2:57 para treinar\n",
        "\n",
        "**Tokenizar o conjunto de dados**\n",
        "\n",
        "A biblioteca Transformers fornece uma função ```encode``` que tratará da maioria das etapas de análise e preparação de dados para nós.\n",
        "\n",
        "Antes de estarmos prontos para codificar nosso texto, precisamos decidir sobre o comprimento máximo de frase para preenchimento ou truncamento.\n",
        "\n",
        "A célula abaixo realizará uma passagem de tokenização em nosso conjunto de dados para medir o comprimento máximo da frase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-bVl31sUThV",
        "outputId": "e1cc7fe2-8452-41b9-d5b2-142f20fae0d6"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# Para cada frase\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokeniza o texto e adiciona os tokens `[CLS]` e `[SEP]`\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Atualiza o comprimento máximo da frase\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Frase com tamanho máximo no nosso dataset: ', max_len)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frase com tamanho máximo no nosso dataset:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4Ph70NljUThW"
      },
      "source": [
        "Para o caso de haver algumas sentenças mais longas no conjunto de teste, vams setar nosso comprimento máximo em 64.\n",
        "\n",
        "Agora estamos prontos para realizar a tokenização.\n",
        "\n",
        "A função ```tokenizer.encode_plus``` realiza várias etapas para nós:\n",
        "\n",
        "1. Divide a frase em *tokens*.\n",
        "2. Adiciona os *tokens* especiais [CLS] e [SEP].\n",
        "3. Mapeia os *tokens* para seus IDs.\n",
        "4. Preenche ou trunca todas as frases com o mesmo comprimento.\n",
        "5. Cria as máscaras de atenção que diferenciam explicitamente *tokens* reais de *tokens* [PAD].\n",
        "\n",
        "As primeiras quatro etapas são realizadas pela função ```tokenizer.encode```, mas vamos usar a ```tokenizer.encode_plus``` para a quinta etapa (máscaras de atenção). [Documentação aqui](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-99KsnSRUThW",
        "outputId": "3a90651d-b8ef-4d6d-e79c-b05723da4f23"
      },
      "source": [
        "# Tokeniza todas as frases e mapeia os tokens em seus IDs\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# Para cada frase\n",
        "for sent in sentences:\n",
        "    # `encode_plus` fará o seguinte:\n",
        "    #   (1) Tokeniza a frase\n",
        "    #   (2) Adiciona o token `[CLS]` no inicio\n",
        "    #   (3) Adiciona o token `[SEP]` no final\n",
        "    #   (4) Mapeia os tokens em seus IDs.\n",
        "    #   (5) Adiciona preenchimento (pad) ou trunca a frase atá o comprimento máximo (`max_length`)\n",
        "    #   (6) Cria máscara de atenção para os tokens [PAD].\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Frase a ser codificada\n",
        "                        add_special_tokens = True, # Adiciona '[CLS]' e '[SEP]'\n",
        "                        max_length = 64,           # Preenche & trunca todas as frases\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Constrói máscaras de atenção\n",
        "                        return_tensors = 'pt',     # Returna tensores pytorch.\n",
        "                   )\n",
        "    \n",
        "    # Adiciona a frase codificada na lista\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # E sua máscara de atenção (simplesmente diferencia tokens reais de tokens de preenchimento - PAD).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Converte a lista em tensores\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Imprime frase 0, agora como uma lista de IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
            "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LpEzUITwUThX"
      },
      "source": [
        "### Divisão dos dados - treinamento e validação\n",
        "\n",
        "Precisamos agora dividir nosso conjunto de treinamento, em 90% para treinamento e 10% para validação.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbziTD_xUThX",
        "outputId": "368c2ae2-bb8d-473b-dcde-26c42d5c9a22"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combina as entradas de treinamento em um TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Criar uma divisão 90-10 para treinamento-validação.\n",
        "\n",
        "# Calcula o número de instâncias para incluir em cada divisão\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide o dataset pegando randomicamente as instância \n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} instâncias de treinamento'.format(train_size))\n",
        "print('{:>5,} instâncias de validação'.format(val_size))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 instâncias de treinamento\n",
            "  856 instâncias de validação\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11MFCzUbUThY"
      },
      "source": [
        "Vamos criar um iterador para nosso conjunto de dados usando a classe *DataLoader*, para economizar memória durante o treinamento, visto que, ao contrário de um *loop* com *for*, com o iterador o conjunto de dados inteiro não precisa ser carregado na memória."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFdSvaIyUThY"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# O DataLoader precisa saber no tamanho do batch size para treinamento, então vamos espeficiar\n",
        "# Para o fine-tuning em uma tarefa, os autores recomendam um batch size de 16 ou 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Criar os DataLoaders para nossos conjuntos de treinamento e teste\n",
        "# Vamos pegar instâncias de treinamento em ordem aleatória\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # Exemplos de treinamento\n",
        "            sampler = RandomSampler(train_dataset), # Seleciona batches aleatoriamente\n",
        "            batch_size = batch_size # Treina com este batch size.\n",
        "        )\n",
        "\n",
        "# Para validação, a ordem não importa, então vamos mantê-la sequencial\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # Exemplos de validação.\n",
        "            sampler = SequentialSampler(val_dataset), # Executa os batches sequencialmente.\n",
        "            batch_size = batch_size # Avalia com este batch size.\n",
        "        )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF0XtfxIUThY"
      },
      "source": [
        "## Modelo de classificação\n",
        "\n",
        "Agora que nossos dados de entrada estão formatados corretamente, é hora de especializar o modelo BERT.\n",
        "\n",
        "### BertForSequenceClassification\n",
        "\n",
        "Para a tarefa de classificação, vamos modificar o modelo BERT pré-treinado para fornecer saídas para classificação e, em seguida, treinar todo modelo em nosso conjunto de dados até que esteja especializado para nossa tarefa.\n",
        "\n",
        "Felizmente, a implementação *Pytorch* do Hugging Face inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PLN. Embora essas interfaces sejam todas construídas em cima de um modelo BERT treinado, cada uma tem diferentes camadas superiores e tipos de saída projetados para nossa tarefa PLN específica.\n",
        "\n",
        "Aqui está a lista atual de classes fornecidas para a especialização da tarefa (ajuste fino ou *fine tuning*):\n",
        "\n",
        "1. BertModel\n",
        "1. BertForPreTraining\n",
        "1. BertForMaskedLM\n",
        "1. BertForNextSentencePrediction\n",
        "1. BertForSequenceClassification (**usaremos aqui**)\n",
        "1. BertForTokenClassification\n",
        "1. BertForQuestionAnswering\n",
        "\n",
        "A documentação para eles pode ser encontrada [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Estaremos usando *BertForSequenceClassification*, modelo BERT com uma camada linear única adicionada no topo para classificação, que será usado como um classificador de frases. A medida que alimentamos os dados de entrada, todo o modelo BERT pré-treinado e a camada adicional de classificação não treinada serão treinados em nossa tarefa específica.\n",
        "\n",
        "OK, vamos carregar o BERT! Existem alguns modelos diferentes de BERT pré-treinados disponíveis. “bert-base-uncased”, modell em inglês, significa a versão que tem apenas letras minúsculas (“sem caixa”) e é a versão menor das duas (“base” vs “large”).\n",
        "\n",
        "A documentação do método ```from_pretrained``` pode ser encontrada [aqui](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), com os parâmetros adicionais definidos [aqui](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ1XxRwlUThY",
        "outputId": "9534de38-4405-4b4e-da69-012c118a33ee"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Carrega a classe BertForSequenceClassification, o modelo pre-treinado com uma camada linear simples no topo\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Usa o BERT com 12 camadas, com vocabulario com caixa baixa\n",
        "    num_labels = 2, # O número de saídas, ou rótulos, do nosso modelo (classificação binária em nosso caso)\n",
        "                    # Para tarefas com mais classes (multi-classe), podemos aumentar esse número\n",
        "    output_attentions = False, # Se o modelo deve retornar os pesos de atenção\n",
        "    output_hidden_states = False, # Se o modelo deve retornar todos estados escondidos\n",
        ")\n",
        "\n",
        "# Se você tem GPU, configura o pytorch para executar o modelo na GPU\n",
        "model.cuda()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGO0u1gjUThZ"
      },
      "source": [
        "Apenas por curiosidade, podemos navegar por todos os parâmetros do modelo.\n",
        "\n",
        "Na célula abaixo, imprimimos os nomes e dimensões dos pesos para:\n",
        "\n",
        "1. A camada de incorporação.\n",
        "1. O primeiro dos doze transformadores.\n",
        "1. A camada de saída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xnrLqqkUThZ",
        "outputId": "9aafea18-6c67-42d6-9856-2f07f903e84f"
      },
      "source": [
        "# Lista todos os paremtros do modelo como uma lista de tuplas\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('O modelo BERT tem {:} parâmetros diferentes.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Camada de incorporação (Embedding) ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Primeiro Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Camada de Saída ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O modelo BERT tem 201 parâmetros diferentes.\n",
            "\n",
            "==== Camada de incorporação (Embedding) ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== Primeiro Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Camada de Saída ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBWGUmgUThZ"
      },
      "source": [
        "## Otimizador e taxas de aprendizagem ##\n",
        "\n",
        "Agora que carregamos nosso modelo, precisamos pegar os hiperparâmetros de treinamento de dentro do modelo armazenado.\n",
        "\n",
        "Para fins de ajuste fino, os autores recomendam escolher entre os seguintes valores (do Apêndice A.3 do artigo de BERT ):\n",
        "\n",
        "1. Tamanho do lote (*batch size*): 16, 32\n",
        "1. Taxa de aprendizagem (*Adam*): 5e-5, 3e-5, 2e-5\n",
        "1. Número de épocas: 2, 3, 4\n",
        "\n",
        "Aqui vamos usar:\n",
        "\n",
        "1. Tamanho do lote: 32 (definido ao criar nossos *DataLoaders*)\n",
        "1. Taxa de aprendizagem: 2e-5\n",
        "1. Épocas: 4 (veremos que provavelmente são muitas ...)\n",
        "\n",
        "O parâmetro ```eps = 1e-8``` é “um número muito pequeno para evitar qualquer divisão por zero na implementação” (leia mais [aqui](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "Você pode encontrar a criação do otimizador *AdamW* em ```run_glue.py``` [aqui](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH48Pnl1UThZ"
      },
      "source": [
        "# Nota: AdamW é uma classe da biblioteca huggingface\n",
        "# Provavelmente o 'W' é de 'Weight Decay fix\" ('Correção de redução de peso \")\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - o padrão é  5e-5, aqui usamos 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - o padrão é 1e-8.\n",
        "                )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yk6BlpEUThZ"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Número de épocas de treinamento. Os autores do BERT recomendam entre 2 a 4. \n",
        "# Nós escolhemos 4, mas vamos ver depois que isso pode estar causando overfit nos dados de treinamento\n",
        "epochs = 4\n",
        "\n",
        "# Número total de passos de treinamento é [número de batches] x [número de épocas]. \n",
        "# (Note que não é o mesmo que o número de instâncias de treinamento).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Criando o programador de taxa de aprendizagem (learning rate scheduler)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Valor padrão no run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0zVkdGUTha"
      },
      "source": [
        "## *Loop* de treinamento ##\n",
        "\n",
        "Abaixo está o nosso *loop* de treinamento. Há muita coisa acontecendo, mas fundamentalmente para cada passagem em nosso *loop*, temos uma fase de treinamento e uma fase de validação.\n",
        "\n",
        "**Treinamento:**\n",
        "\n",
        "1. Descompacta nossas entradas (dados e rótulos)\n",
        "1. Carrega dados na GPU para aceleração (quando necessário)\n",
        "1. Limpa os gradientes calculados na passagem anterior\n",
        "(no *pytorch*, os gradientes se acumulam por padrão (útil para RNNs), a menos que a gente limpe explicitamente)\n",
        "1. Passo *Forward* (avanço), alimentando os dados de entrada pela rede\n",
        "1. Passo *Backward* (para trás), ou retropropagação\n",
        "1. Informa a rede para atualizar os parâmetros com ```optimizer.step ()```\n",
        "1. Rastreia variáveis para monitorar o progresso\n",
        "\n",
        "**Avaliação:**\n",
        "\n",
        "1. Descompacta nossas entradas (dados e rótulos)\n",
        "1. Carrega dados na GPU para aceleração (quando necessário)\n",
        "1. Passo *Forward* (avanço), alimentando os dados de entrada pela rede\n",
        "1. Calcula a perda (*loss*) em nossos dados de validação e rastreia variáveis para monitorar o progresso\n",
        "\n",
        "*Pytorch* esconde todos os cálculos detalhados, mas comentamos o código para apontar quais das etapas acima estão acontecendo em cada linha.\n",
        "\n",
        "No código abaixo, definimos uma função auxiliar para calcular a precisão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSljX7FAUTha"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Função para caluclar a acurácia das nossas perdições x rótulos\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDjNSgJjUTha"
      },
      "source": [
        "Função auxiliar para formatar o tempo decorrido (hh:mm:ss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEsSXtJCUTha"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Pega o tempo em segundos e retorna como hh:mm:ss\n",
        "    '''\n",
        "    # Arredonda\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Formata como hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Rur-k-UTha"
      },
      "source": [
        "Estamos prontos para começar o treinamento!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzk4DAAgUThb",
        "outputId": "014c103e-3598-48f4-b0be-71b427941102"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Esse código é baseado no `run_glue.py`\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Seta o valor 'seed' para tornar reproduzível\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Vamos guardar os valores de perda do treinamento e validação, acurária e tempos de execução.\n",
        "training_stats = []\n",
        "\n",
        "# Medir o total do tempo de treinamento para toda execução\n",
        "total_t0 = time.time()\n",
        "\n",
        "# para cada época\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Treinamento\n",
        "    # ========================================\n",
        "    \n",
        "    # Executa um passo inteiro em todo conjunto de treinamento\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Época {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Treinando...')\n",
        "\n",
        "    # Mede quando tempo a época de treinamento demora\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reseta a perda total para esta época\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Coloca o modelo em modo de treinamento.\n",
        "    # Essa chamada para `train` apenas muda o modo, mas não executa o treinamento.\n",
        "    # as camadas `dropout` e `batchnorm` tem comportamento diferente durante o treinamento\n",
        "    # (veja mais: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # Para cada batch dos nossos dados de treinamento\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Atualiza progresso a cada 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calcula o tempo em minutos.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Imprime o progresso.\n",
        "            print('  Batch {:>5,}  de  {:>5,}.    Tempo: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Retira esse batch de treinamento do nosso dataloader. \n",
        "        #\n",
        "        # Agora que retiramos esse batch, também vamos copiar cada tensor para a GPU usando o método `to`, se necessário.\n",
        "        #\n",
        "        # `batch` contém três tensores pytorch:\n",
        "        #   [0]: ids de entrada \n",
        "        #   [1]: máscaras de atenção\n",
        "        #   [2]: rótulos\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Sempre limpe qualquer gradiente calculado anteriormente antes de executar o passo de \n",
        "        # backward, pois o PyTorch não faz isso automaticamente, já que acumular os gradientes é\n",
        "        # importante para o treinamento das RNNs. \n",
        "        # (https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Executa o passo forward (avalia o modelo no batch de treinamento).\n",
        "        # Documentação para essa função: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # Retorna números diferentes de parâmetros, dependendo de qual argumento foi passado e quais flags foram setadas.\n",
        "        # Aqui, vai retornar a perda (loss) e os \"logits\", saídas do modelo antes da ativação\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Acumula a perda do treinamento de todos os batches então podemos calcular a média da perda no final. \n",
        "        # `loss` é um Tensor contendo um valor único, a função `.item()` apenas retorna o valor Python do tensor.\n",
        "        \n",
        "        # com a versão nova do Transformers (4.x) aqui dá erro\n",
        "        total_train_loss += loss.item()\n",
        "        \n",
        "        # Executa o passo backward para calcular os gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Corta a norma dos gradientes para 1.0., para ajudar a prevenir o problema de \"exploding gradients\".\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Atualiza parâmetros e passa uma etapa usando o gradiente calculado.\n",
        "        # O otimizador dita o \"update rule\", ou seja, como os parâmetros são modificados com base \n",
        "        # nos gradientes, taxa de aprendizagem, etc\n",
        "        optimizer.step()\n",
        "\n",
        "        # Atualiza taxa de aprendizagem\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calcula a perda média sobre todos os batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Mede quanto tempo levou essa época\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Perda média do treinamento: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  época de treinamento levou : {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validação\n",
        "    # ========================================\n",
        "    # Após a conclusão de cada época de treinamento, vamos medir nosso desempenho em nosso conjunto de validação\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Executando Validação...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Coloca o modelo no modo de avaliação, ou seja, as camadas dropout terão comportamento diferente\n",
        "    model.eval()\n",
        "\n",
        "    # Variáveis de rastreamento\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Avalia os dados para uma época\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Retira esse batch de treinamento do nosso dataloader. \n",
        "        #\n",
        "        # Agora que retiramos o batch, vaos copiar cada tensor para nossa GPU com o método `to`, se estamos usando GPU\n",
        "        #\n",
        "        # `batch` contém 3 tensores pytorch:\n",
        "        #   [0]: ids de entrada \n",
        "        #   [1]: máscaras de atenção\n",
        "        #   [2]: rótulos\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Informa pytorch para não não se preocupar em construir o gráfico de computação durante o passo forward,\n",
        "        # já que precisa apenas no passo backprop (treinamento).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Passo Forward, calcula as predições logit.\n",
        "            # token_type_ids é o mesmo que os \"segment ids\", que diferencia as sentenças 1 e 2 em uma tarefa com 2 sentenças\n",
        "            # Documentação da função `model`: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Pega as saídas \"logits\" do model, valores prévios à aplicação da função de ativação como a softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Acumula a perda da validação\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move os logits e rótulos para CPU, se necessário\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calcula a acurácia desse batch nas frases de teste, e acumula sobre todos os batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Imprime a acurácia final para a execução da validação.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Acurácia: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calcula a média da perda sobre todos os batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Mede quanto tempo levou a validação\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validação perda (Loss): {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validação levou: {:}\".format(validation_time))\n",
        "\n",
        "    # Grava as estatísticas para esta época.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Treinamento completo!\")\n",
        "\n",
        "print(\"Tempo total de treinamento {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Época 1 / 4 ========\n",
            "Treinando...\n",
            "  Batch    40  de    241.    Tempo: 0:00:14.\n",
            "  Batch    80  de    241.    Tempo: 0:00:28.\n",
            "  Batch   120  de    241.    Tempo: 0:00:42.\n",
            "  Batch   160  de    241.    Tempo: 0:00:57.\n",
            "  Batch   200  de    241.    Tempo: 0:01:11.\n",
            "  Batch   240  de    241.    Tempo: 0:01:27.\n",
            "\n",
            "  Perda média do treinamento: 0.49\n",
            "  época de treinamento levou : 0:01:27\n",
            "\n",
            "Executando Validação...\n",
            "  Acurácia: 0.83\n",
            "  Validação perda (Loss): 0.40\n",
            "  Validação levou: 0:00:04\n",
            "\n",
            "======== Época 2 / 4 ========\n",
            "Treinando...\n",
            "  Batch    40  de    241.    Tempo: 0:00:16.\n",
            "  Batch    80  de    241.    Tempo: 0:00:32.\n",
            "  Batch   120  de    241.    Tempo: 0:00:47.\n",
            "  Batch   160  de    241.    Tempo: 0:01:02.\n",
            "  Batch   200  de    241.    Tempo: 0:01:18.\n",
            "  Batch   240  de    241.    Tempo: 0:01:33.\n",
            "\n",
            "  Perda média do treinamento: 0.30\n",
            "  época de treinamento levou : 0:01:34\n",
            "\n",
            "Executando Validação...\n",
            "  Acurácia: 0.84\n",
            "  Validação perda (Loss): 0.46\n",
            "  Validação levou: 0:00:04\n",
            "\n",
            "======== Época 3 / 4 ========\n",
            "Treinando...\n",
            "  Batch    40  de    241.    Tempo: 0:00:16.\n",
            "  Batch    80  de    241.    Tempo: 0:00:31.\n",
            "  Batch   120  de    241.    Tempo: 0:00:46.\n",
            "  Batch   160  de    241.    Tempo: 0:01:02.\n",
            "  Batch   200  de    241.    Tempo: 0:01:17.\n",
            "  Batch   240  de    241.    Tempo: 0:01:33.\n",
            "\n",
            "  Perda média do treinamento: 0.19\n",
            "  época de treinamento levou : 0:01:33\n",
            "\n",
            "Executando Validação...\n",
            "  Acurácia: 0.83\n",
            "  Validação perda (Loss): 0.57\n",
            "  Validação levou: 0:00:04\n",
            "\n",
            "======== Época 4 / 4 ========\n",
            "Treinando...\n",
            "  Batch    40  de    241.    Tempo: 0:00:15.\n",
            "  Batch    80  de    241.    Tempo: 0:00:31.\n",
            "  Batch   120  de    241.    Tempo: 0:00:46.\n",
            "  Batch   160  de    241.    Tempo: 0:01:02.\n",
            "  Batch   200  de    241.    Tempo: 0:01:17.\n",
            "  Batch   240  de    241.    Tempo: 0:01:33.\n",
            "\n",
            "  Perda média do treinamento: 0.13\n",
            "  época de treinamento levou : 0:01:33\n",
            "\n",
            "Executando Validação...\n",
            "  Acurácia: 0.84\n",
            "  Validação perda (Loss): 0.58\n",
            "  Validação levou: 0:00:04\n",
            "\n",
            "Treinamento completo!\n",
            "Tempo total de treinamento 0:06:21 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vyJO3kOUThb"
      },
      "source": [
        "Vamos ver o resumo do processo de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IQYHD9T8UThb",
        "outputId": "f17399fd-57cf-4238-a5a3-940c03bdbcf3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mostra números com duas casas decimais\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Cria um DataFrame das nossas estatísticas de treinamento\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Usa a época como o índice da linha\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# Forçar o agrupamento dos cabeçalho da coluna \n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Mostra a tabela\n",
        "df_stats"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0:01:27</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:01:34</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0:01:33</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:01:33</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.49         0.40           0.83       0:01:27         0:00:04\n",
              "2               0.30         0.46           0.84       0:01:34         0:00:04\n",
              "3               0.19         0.57           0.83       0:01:33         0:00:04\n",
              "4               0.13         0.58           0.84       0:01:33         0:00:04"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guNi39XGUThb"
      },
      "source": [
        "Observe que, enquanto a perda de treinamento está diminuindo a cada época, a perda de validação está aumentando! Isso sugere que estamos treinando nosso modelo por muito tempo e que ele está se ajustando demais aos dados de treinamento.\n",
        "\n",
        "(Para referência, estamos usando 7.695 amostras de treinamento e 856 amostras de validação).\n",
        "\n",
        "A perda de validação é uma medida mais precisa do que a precisão, porque com a precisão não nos importamos com o valor de saída exato, mas apenas em que lado de um limite ele cai.\n",
        "\n",
        "Se estivermos prevendo a resposta correta, mas com menos confiança, a perda de validação pegará isso, mas a precisão não."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "J3OyoiaCUThb",
        "outputId": "8c2b788f-4c46-4192-f5c4-8eb09e602391"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Usando estilo\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Aumentando o tamanho e fonte \n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plotando a curva de aprendizagem\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Treinamento\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validação\")\n",
        "\n",
        "# Adicionando títulos\n",
        "plt.title(\"Perda de treinamento e validação\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjyahGzSUThc"
      },
      "source": [
        "## Desempenho no conjunto de teste \n",
        "\n",
        "Agora, carregaremos o conjunto de dados *holdout* e preparamos as entradas, como fizemos com o conjunto de treinamento. \n",
        "\n",
        "Em seguida, avaliaremos as previsões usando o [coeficiente de correlação de Matthew](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html), métrica usada para avaliar o desempenho no CoLA. Com essa métrica, +1 é a melhor pontuação e -1 é a pior pontuação. Dessa forma, podemos ver nosso desempenho em relação aos modelos de última geração para essa tarefa específica.\n",
        "\n",
        "### Preparação de dados \n",
        "\n",
        "Precisaremos aplicar todas as mesmas etapas que fizemos para os dados de treinamento para preparar nosso conjunto de dados de teste.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4GZU-RGUThc",
        "outputId": "5a14d864-b278-4fa4-b24a-e8a120bbed00"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carrega o dataset em um dataframe pandas\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Imprime o número de frases\n",
        "print('Número de frases de teste: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Cria as listas de frases e rótulos\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokeniza todas as sentenças e mapeia os tokens em seus IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# Para cada frase\n",
        "for sent in sentences:\n",
        "    # `encode_plus` vai:\n",
        "    #   (1) Tokenizar a frase\n",
        "    #   (2) Adicionar o token `[CLS]` no início\n",
        "    #   (3) Adicionar o token `[SEP]` no final.\n",
        "    #   (4) Mapear tokens aos seus IDs.\n",
        "    #   (5) Preencher ou truncar a frase até `max_length`\n",
        "    #   (6) Criar máscara de atenção para os tokens [PAD].\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentença a ser codificada\n",
        "                        add_special_tokens = True, # Adiciona '[CLS]' e '[SEP]'\n",
        "                        max_length = 64,           # Preenche & trunca todas as sentenças\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Constrói máscara de atenção\n",
        "                        return_tensors = 'pt',     # Retorna tensores pytorch.\n",
        "                   )\n",
        "    \n",
        "    # Adiciona a sentença codificada na lista\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # E sua máscara de atenção (simplesmente diferencia preenchimento (PAD) de não-pad).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Converte as listas em tensores.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Seta o batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Cria o DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de frases de teste: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeqtTUuWUThc"
      },
      "source": [
        "### Avaliação no conjunto de teste ###\n",
        "\n",
        "Com o conjunto de teste preparado, podemos aplicar nosso modelo ajustado para gerar previsões no conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOO9WqCkUThc",
        "outputId": "ef6f533a-a422-4a03-daad-7e0ca2c5e1bf"
      },
      "source": [
        "# Predição no conjunto de teste\n",
        "\n",
        "print('Predição de rótulos para {:,} frases de teste...'.format(len(input_ids)))\n",
        "\n",
        "# coloca o modele em modo de avaliação\n",
        "model.eval()\n",
        "\n",
        "# Variáveis de rastreamento\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predição \n",
        "for batch in prediction_dataloader:\n",
        "  # Adiciona o batch na GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Retira as entradas do dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Informa o modelo para não computar gradientes, salvando memória e acelerando a predição \n",
        "  with torch.no_grad():\n",
        "      # Passo Forward, calcula as predições logit\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move as logits e rótulos para CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Armazen predições e rótulos verdadeiros\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('FIM.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predição de rótulos para 516 frases de teste...\n",
            "FIM.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diwh1xgqUThc"
      },
      "source": [
        "A precisão no *benchmark* CoLA é medida usando o “coeficiente de correlação de Matthews” (MCC).\n",
        "\n",
        "Usamos MCC aqui porque as classes são desequilibradas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss3ueMk5UThd",
        "outputId": "6042df38-2cb2-4919-feba-f259fee43221"
      },
      "source": [
        "print('Instâncias positivas: %d de %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instâncias positivas: 354 de 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP02zqm4UThd",
        "outputId": "60feb3eb-64bf-4c36-9cbe-fe1647079ba9"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Avaliando cada batch de teste usando MCC\n",
        "print('Calculando MCC para cada batch...')\n",
        "\n",
        "# Para cada batch\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # As predições para este batch são a 2a. coluna do ndarray (uma coluna para \"0\" e outra para \"1\")\n",
        "  # Pega o rótulo com o maior valor e transforma em uma lista de 0s e 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calcula e armazena o coef para este batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculando MCC para cada batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNnOntTjUThd"
      },
      "source": [
        "A pontuação final será baseada em todo o conjunto de teste, mas vamos dar uma olhada nas pontuações dos lotes individuais para ter uma noção da variabilidade da métrica entre os lotes.\n",
        "\n",
        "Cada lote contém 32 sentenças, exceto o último lote que contém apenas (516% 32) = 4 sentenças de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "2FdSg9ZVUThd",
        "outputId": "b0a57176-f9c9-45c0-fd91-db93b2bee650"
      },
      "source": [
        "# Cria um barplot mostrando o MCC para cada batch dos nossos exemplos de teste.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC por Batch')\n",
        "plt.ylabel('MCC Score (-1 a +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIprQ7dIUThe"
      },
      "source": [
        "Agora combinaremos os resultados de todos os lotes e calcularemos nossa pontuação final no MCC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgg1F-lCUThe",
        "outputId": "14162985-fa56-4e49-df3e-8e3aea3bb5a4"
      },
      "source": [
        "# Combina os resultados para todos os batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# Para cada exemplo, pega o rótulo (0 ou 1) com maior score\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combina os rótulos para cada batch em uma única lista.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calcula o MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G2SYdJSUThe"
      },
      "source": [
        "Legal! Em cerca de meia hora e sem fazer nenhum ajuste de hiperparâmetro (ajustando a taxa de aprendizagem, épocas, tamanho do lote, propriedades ADAM, etc.), obtivemos uma boa pontuação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wbeNbEZUThe"
      },
      "source": [
        "Nota: Para maximizar a pontuação, devemos remover o “conjunto de validação” (que usamos para ajudar a determinar em quantas épocas treinar) e treinar em todo o conjunto de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AddBAXzdUThe"
      },
      "source": [
        "A biblioteca documenta a precisão esperada para este *benchmark* aqui como **49.23** ([ver tabela de classificação oficial](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy)).\n",
        "\n",
        "## Conclusão \n",
        "\n",
        "Com um modelo BERT pré-treinado, podemos criar de forma rápida e eficaz um modelo de alta qualidade com o mínimo de esforço e tempo de treinamento usando a interface *Pytorch*, independentemente da tarefa PLN específica em que estamos interessados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2vM0vX2UThe"
      },
      "source": [
        "### Salvar e carregar o modelo ajustado \n",
        "\n",
        "A próxima célula grava o modelo e o tokenizador no disco."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2clVnjW1UThf",
        "outputId": "2a2a0a6a-57e7-4d00-db46-1b5504de01f1"
      },
      "source": [
        "import os\n",
        "\n",
        "# Boa-prática: usar os nomes padrão dos modelos, para fazer load usando from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Criar diretório de saída se necessário\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Savando modelo em %s\" % output_dir)\n",
        "\n",
        "# Salva um modelo treinado, sua configuração e tokenizador com `save_pretrained()`.\n",
        "# Eles podem ser carregados com `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Cuida do treinamento paralelo/distribuído\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Boa-prática: salve os argumentos de treinamento junto com o modelo\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Savando modelo em ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcE3HTvOUThf"
      },
      "source": [
        "Vamos verificar os tamanhos dos arquivos, por curiosidade.\n",
        "\n",
        "No Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g47ZXApIUThf",
        "outputId": "9454ad83-5111-412a-b3f7-129e48b5a5fb"
      },
      "source": [
        "!ls -l --block-size=K ./model_save/\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427996K\n",
            "-rw-r--r-- 1 root root      1K Dec 11 19:55 config.json\n",
            "-rw-r--r-- 1 root root 427753K Dec 11 19:55 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K Dec 11 19:55 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K Dec 11 19:55 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K Dec 11 19:55 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dC-d_SUThf"
      },
      "source": [
        "O maior arquivo é o peso do modelo, em torno de 418 megabytes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF7Jc6cEUThf",
        "outputId": "870cc3c9-b3c1-481e-8a05-36ffd43cddb3"
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M Dec 11 19:55 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fONYFF4TUThf"
      },
      "source": [
        "Para salvar seu modelo nas sessões do **Colab Notebook**, baixe-o em sua máquina local ou, de preferência, copie-o para o Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdXVUpBLY2tz"
      },
      "source": [
        "# Montando Google Drive nesta instância\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNzuRt1OYM_b"
      },
      "source": [
        "# Copia os arquivos do modelo num diretório do Google Drive.\r\n",
        "!cp -r ./model_save/ \"./drive/Shared drives/PLN/BERT Fine-Tuning/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8UjaufMUThg"
      },
      "source": [
        "As funções a seguir carregarão o modelo de volta do disco.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUQgKi5OUThg"
      },
      "source": [
        "# Carrega um modelo treinado e seu vocabulário\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copia o modelo na GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
